<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>paper reading: Nested Attention: Semantic-aware Attention Values for Concept Personalization-2025</title>
    <link href="/paper-reading/paper-reading/"/>
    <url>/paper-reading/paper-reading/</url>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://snap-research.github.io/NestedAttention/">文章链接</a></p></blockquote><p>关键概括：injects a rich and expressive image representation into the model’s existing cross-attention layers. &#x3D;&#x3D;&gt;  <strong>single textual-token</strong>、<strong>cross-attention: Nested Attention</strong>、 <strong>smaller semantic visual elements</strong>、 <strong>多个定制化概念</strong>、<strong>可以非人 数据集低需求</strong>  </p><p>personalization技术发展路线：text-embedding&#x2F;fine-tuning based——&gt;cross-image attention based——&gt;<em><strong>encoder-based（本文）</strong></em>✨</p><h2 id="1-如何理解per-query-attention-values"><a href="#1-如何理解per-query-attention-values" class="headerlink" title="1. 如何理解per-query attention values?"></a>1. 如何理解per-query attention values?</h2><h3 id="回顾cross-attention"><a href="#回顾cross-attention" class="headerlink" title="回顾cross-attention"></a>回顾cross-attention</h3><p>Query: hidden_states(来自unet中间层)</p><p>Key &amp; Value: text_embedding(来自text-encoder)</p><p>Q 与 K 的点积意义：表示当前空间位置下的 Q_ij 与 K 的语义相似性，即权重，用来后续与 V 加权。由此，在给定文本（K &#x2F; V ）下，Q的能够决定图像内容的“空间位置”，也就是控制了图像的“外观”！这是文章着重研究 Query 的原因。</p><p><img src="/../images/paper-reading/image-20250108160404745.png" alt="cross-attention" title="cross-attention"></p><h3 id="per-query-attention-Values"><a href="#per-query-attention-Values" class="headerlink" title="per-query attention Values"></a>per-query attention Values</h3><p>原文提取：per-query attention Values &#x3D; localized values that depend on the queries&#x3D;per-region values&#x3D;query-dependent values</p><p>Value: text_embedding(来自text-encoder)，由于 Value由不同的token对应的embedding组成，而一个token却要指示着图片<em>整个区域</em>的全部相关实例和相关内容，这很“粗粒度”，无法达到任务期待的“细粒度”个性化生成【value中的每一个embedding要负责整个query，任务重，容易完成的不好】。因此把任务细分：划分query，每个子query由专门的新value负责，减轻了value的任务量。即：提出了更局部的“localized Values”：能更好的关注到局部区域、细粒度的语义信息。</p><p>所谓的“per-query attention Values”具体是怎样实现的就是下面的内容了。</p><blockquote><p>注意：per-query attention Values ≠ attention map的值，Values指的是Q K V中的V。</p></blockquote><h2 id="2-如何理解nested-attention-mechanism？"><a href="#2-如何理解nested-attention-mechanism？" class="headerlink" title="2. 如何理解nested attention mechanism？"></a>2. 如何理解nested attention mechanism？</h2><p>其中公式1就是上文的“per-query attention Values”的实现方式了。简单来说，就是对special token（s*）在不同的空间位置（i,j）下的q_ij，单独预测value_ij，这样得到的value_ij便具有了更局部的、细粒度的语义信息。但不是所有的text token都是用这个机制，只有要被个性化的special token会用到，这种注意力机制就是nested attention mechanism。</p><p><img src="/../images/paper-reading/image-20250108165121686.png" alt="公式1&#x2F;2&#x2F;3" title="公式1&#x2F;2&#x2F;3"></p><blockquote><p>注意，Key 在公式1和3中的区别！(从左到右分别：公式1&#x2F;2&#x2F;3)<br>文中还提到了对“per-query attention Values”的正则化实验技巧，不具体介绍。</p></blockquote><h2 id="3-q-ij、nested-keys、nested-values从哪来？——可训练模块"><a href="#3-q-ij、nested-keys、nested-values从哪来？——可训练模块" class="headerlink" title="3. q_ij、nested keys、nested values从哪来？——可训练模块"></a>3. q_ij、nested keys、nested values从哪来？——可训练模块</h2><p>Q-Former得到：”Q-Former learned queries“，即q_ij；</p><p>nested attention layers[ linear layers ]得到：nested keys、nested values；</p><p>上述两个模块组成了文章的可训练部分，得到的q_ij、nested keys、nested values三者构成公式1的输入。</p><blockquote><p>注意：per-query attention Values ≠ nested values, 二者关系：nested values 和 nested keys 经过公式1 得到per-query attention Values。</p><p>clip image features &#x3D; CLIP ‘s last layer before pooling</p></blockquote><p><img src="/../images/paper-reading/image-20250108172150828.png" alt="论文架构" title="论文架构"></p><h2 id="4-对“Q-Former-learned-queries”的验证："><a href="#4-对“Q-Former-learned-queries”的验证：" class="headerlink" title="4. 对“Q-Former learned queries”的验证："></a>4. 对“Q-Former learned queries”的验证：</h2><p>从生成过程中的Query中取3个不同空间位置的q_ij，与nested keys进行点积运算得到attention map’，可以观察到总能有1-2个nested token与q_ij最相关；进一步将q_ij、nested keys、nested values按照公式1进行运算，得到Q-Former learned queries，与输入脸部图像的clip image features 进行点积运算得到attention map, 能直观的观察到Q-Former learned queries的作用，即生成的细粒度特征在输入图中的来源相关性。</p><p><img src="/../images/paper-reading/image-20250108174006292.png" alt="可视化验证" title="可视化验证"></p>]]></content>
    
    
    <categories>
      
      <category>paper_reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>attention diffusion text-to-img personalization 论文阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>recommends</title>
    <link href="/live/my_attitude/"/>
    <url>/live/my_attitude/</url>
    
    <content type="html"><![CDATA[<p>影视剧、书籍、音乐和生活方式推荐</p><p>下面会介绍我推荐的、我最近在看的影视剧、书籍、音乐和生活方式，说实话我认为这种“生活态度”的安利是极具私人性的，毕竟，它们组成了我个人。不过我不介意，因为我更看重它们对我来说的其他意义：记忆和成长。因此我会持续更新的~ </p><p>不论是想要了解我还是想要接受我的安利，都请继续关注吧！ </p><h2 id="影视"><a href="#影视" class="headerlink" title="影视"></a>影视</h2><h3 id="电影"><a href="#电影" class="headerlink" title="电影"></a>电影</h3><ul><li><a href="https://movie.douban.com/subject/36445098/">还有明天</a> 2023<br>观影过程中以为和国内<a href="https://movie.douban.com/subject/36587974/">出走的决心-2024</a>剧情类似，结果！我还是太狭隘了！！强烈推荐</li><li><a href="https://movie.douban.com/subject/26656728/">老娘与海 <strong>又名：泳者之心</strong></a> 2024</li><li><a href="https://movie.douban.com/subject/25821498/">妇女参政论者</a> 2015</li></ul><h3 id="连续剧"><a href="#连续剧" class="headerlink" title="连续剧"></a>连续剧</h3><ul><li><a href="https://movie.douban.com/subject/1474087/">无耻之徒</a> 2004-2020 <strong>忽略frank的不负责任真的是最爱frank！</strong></li><li><a href="https://book.douban.com/subject/27204805/">我的天才女友(意大利)</a> 2018-2024<br>关键词：书籍《那不勒斯四部曲》改编：我的天才女友、新名字的故事、离开的，留下的、失踪的孩子<br>很细腻的女性作家书写的两名女性的友谊：复杂、既有爱又有恨和期待；很喜欢两个女生以各自的速度成长。</li><li><a href="https://movie.douban.com/subject/26838164/">伦敦生活</a> 2016-2019</li><li><a href="https://movie.douban.com/subject/36085524/">影后(台湾)</a> 2024 <strong>太爱杨谨华！</strong></li></ul><h3 id="动漫"><a href="#动漫" class="headerlink" title="动漫"></a>动漫</h3><ul><li>刺客伍六七：超级轻松搞笑！！！</li></ul><h2 id="书籍"><a href="#书籍" class="headerlink" title="书籍"></a>书籍</h2><ul><li><a href="https://book.douban.com/subject/25836270/">厌女：日本的女性厌恶——上野千鹤子</a> 2015<br>关键词：男性同性社会性欲望homosocial、厌女misogyny、同性恋憎恶homophobia、厌女症：男性对女性的蔑视和女性的自我厌恶<br>理论性还是比<a href="https://book.douban.com/subject/35523099/?icn=index-latestbook-subject">从零开始的女性主义</a>要高得多，而且翻译感好重，什么时候国内女性主义作家能把<a href="https://book.douban.com/subject/6722209/">男人之间</a>用国内的历史和语言文化，在近代的情况下进行更新的分析呢？希望！</li><li><a href="https://book.douban.com/subject/35966120/?icn=index-topchart-subject">始于极限——上野千鹤子</a> 2022 <strong>正在阅读</strong></li><li><a href="https://book.douban.com/subject/35143790/">蛤蟆先生去看心理医生——罗伯特·戴博德</a> 2020 <strong>第二次阅读</strong></li></ul><h2 id="音乐"><a href="#音乐" class="headerlink" title="音乐"></a>音乐</h2><ul><li>歌手：</li><li>别野加奈：很适合专注时的背景音会，让人平静下来</li></ul><h2 id="生活方式"><a href="#生活方式" class="headerlink" title="生活方式"></a>生活方式</h2><h3 id="播客："><a href="#播客：" class="headerlink" title="播客："></a>播客：</h3><ul><li>文化有限：每期对应一本<strong>书</strong></li><li>凹凸电波：搞笑的一群朋友们闲谈，听起来轻松</li><li>燕外之意：针对某一主题对网友的经历汇总</li><li>思文败类：轻松+小思考</li><li>随机波动：感性、深邃的女性在谈论中思考</li><li>岩中花述：鲁豫的对谈，最近都是”she”她主题的女性嘉宾</li></ul><h3 id="运动："><a href="#运动：" class="headerlink" title="运动："></a>运动：</h3><ul><li>健身：争取引体向上</li><li>攀岩：新时代不分性别的”裹小脚”</li><li>网球：希望对球的控制更稳定一些</li><li>游泳：谁能想到每周都在坚持游泳！</li></ul><p><img src="/../images/roy.jpg" alt="王源图片" title="roy"></p><blockquote><p>github: <a href="https://pljj315.github.io/">pljj315</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>live</category>
      
    </categories>
    
    
    <tags>
      
      <tag>安利 影视剧 书籍 音乐 生活方式</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
