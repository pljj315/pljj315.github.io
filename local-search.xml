<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Inversion_based_editing in flow_matching models</title>
    <link href="/Paper-Reading/Paper-Reading_Inversion_based_%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/"/>
    <url>/Paper-Reading/Paper-Reading_Inversion_based_%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/</url>
    
    <content type="html"><![CDATA[<blockquote><p>前言：基于扩散&#x2F;流生成模型中的图像编辑：</p><ul><li>test-time optimization：规模性的微调训练，如FLUX-Fill 模型、FLUX-redux万物迁移</li><li>optimization-free：</li><li>Inversion-reconstruction：如 RF-Inversion，RF-Edit，并不能算“编辑”，更像“图生图”，适合风格迁移，无法背景保持，<strong>耗时 step*2</strong></li><li>Inversion-free：如 Flow Edit，不再借助反演的中间的高斯噪声分布，但并没有节省时间消耗，依然 <strong>耗时 step*2</strong></li><li>Inversion-based-cache：借助Inversion，并在采样时注入Inversion得到的中间表示【如：序列token，V，k&amp;V，等】，<strong>耗时 step*2</strong>——&gt; 实现结构保留的图像编辑！！！<ul><li>RF-Edit：注意力共享：替换 V【单流自注意力中的V_ref】</li><li>KV-Edit：注意力共享：替换 K &amp; V</li><li>Personalize Anything：图像token替换，位置编码ids的实验探究</li></ul></li></ul></blockquote><p>什么是Inversion? </p><p>扩散的Inversion如何实现？ </p><p>FLow_based的Inversion如何实现？</p><p>阅读博文：</p><ul><li><a href="http://blog.csdn.net/weixin_44966641/article/details/138804404">Diffusion Inversion技术</a></li></ul><p>flux代码：</p><p><code>sigmas = np.linspace(1.0, 1 / T_steps, T_steps) </code>从1~0均匀取值，但是取不到0,最后一位&#x3D;&#x3D;(1&#x2F;T_steps)!!!!</p><p>VAE：16通道，8倍压缩比 &#x3D; pipe.vae_scale_factor&#x3D;8， patchify图块化2*2的两倍压缩比,  FLUX总压缩比16</p><p>加噪：<code>Z_t= (1-t)*Z_0 + t*N_t, N_t~正态分布(0,1)</code>  scale_noise函数中实现；</p><p>去噪：<code>latents = self.scheduler.step( noise_pred, t, latents, return_dict=False )[0]</code></p><p>FluxTransformer2DModel.FluxAttnProcessor2_0个数：57 &#x3D;&#x3D; single_transformer_blocks：38 + transformer_blocks：19</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FluxTransformer2DModel</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">            self,</span><br><span class="hljs-params">            patch_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">            in_channels: <span class="hljs-built_in">int</span> = <span class="hljs-number">64</span>,</span><br><span class="hljs-params">            out_channels: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">            num_layers: <span class="hljs-built_in">int</span> = <span class="hljs-number">19</span>,           <span class="hljs-comment"># 双流MM-DiT :&#x27;transformer_blocks.18.attn.processor&#x27;</span></span><br><span class="hljs-params">            num_single_layers: <span class="hljs-built_in">int</span> = <span class="hljs-number">38</span>,    <span class="hljs-comment"># 单流DiT    :&#x27;single_transformer_blocks.37.attn.processor&#x27;</span></span><br><span class="hljs-params">            attention_head_dim: <span class="hljs-built_in">int</span> = <span class="hljs-number">128</span>,</span><br><span class="hljs-params">            num_attention_heads: <span class="hljs-built_in">int</span> = <span class="hljs-number">24</span>,</span><br><span class="hljs-params">            joint_attention_dim: <span class="hljs-built_in">int</span> = <span class="hljs-number">4096</span>, <span class="hljs-comment"># img_dims</span></span><br><span class="hljs-params">            pooled_projection_dim: <span class="hljs-built_in">int</span> = <span class="hljs-number">768</span>,</span><br><span class="hljs-params">            guidance_embeds: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">            axes_dims_rope: <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>] = (<span class="hljs-params"><span class="hljs-number">16</span>, <span class="hljs-number">56</span>, <span class="hljs-number">56</span></span>), </span>)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">attn_processors</span>()<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">set_attn_processor</span>()<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fuse_qkv_projections</span>()<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">unfuse_qkv_projections</span>()<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FluxTransformerBlock</span>(nn.Module)<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FluxSingleTransformerBlock</span>(nn.Module)<br></code></pre></td></tr></table></figure><h2 id="1-Inversion-in-flow-matching-models"><a href="#1-Inversion-in-flow-matching-models" class="headerlink" title="1. Inversion in flow_matching models:"></a>1. Inversion in flow_matching models:</h2><h3 id="1-1-RF-Inversion：-Semantic-Image-Inversion-and-Editing-using-Rectified-Stochastic-Differential-Equations——谷歌【收入diffusers】"><a href="#1-1-RF-Inversion：-Semantic-Image-Inversion-and-Editing-using-Rectified-Stochastic-Differential-Equations——谷歌【收入diffusers】" class="headerlink" title="1.1 RF-Inversion： Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations——谷歌【收入diffusers】"></a>1.1 RF-Inversion： Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations——谷歌【收入diffusers】</h3><ol><li><p>理论性强，没看懂;</p></li><li><p>被收入diffusers: 在：<code>examples/community/pipeline_flux_rf_inversion.py</code>中的<code>FlowMatchEulerDiscreteSDEScheduler</code></p><blockquote><p>注意：diffusers中<code>FluxPipeline</code>默认的flux采样器是<code>FlowMatchEulerDiscreteScheduler</code></p><p>RF-Inversion需要自己拉仓库导入上述<code>FlowMatchEulerDiscreteSDEScheduler</code></p></blockquote></li><li><p>invert参数固定：gamma&#x3D;0.5，inversion直接利用ref的latent_image_ids</p></li><li><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pipeline.invert</span><br><br><span class="hljs-comment"># Eq 8 dY_t = [u_t(Y_t) + γ(u_t(Y_t|y_1) - u_t(Y_t))]dt</span><br>Y_t = image_latents<br>y_1 = torch.randn_like(Y_t)<br>N = <span class="hljs-built_in">len</span>(sigmas)<br><br><span class="hljs-comment"># forward ODE loop</span><br><span class="hljs-keyword">with</span> <span class="hljs-variable language_">self</span>.progress_bar(total=N - <span class="hljs-number">1</span>) <span class="hljs-keyword">as</span> progress_bar:<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N - <span class="hljs-number">1</span>):<br>        t_i = torch.tensor(i / (N), dtype=Y_t.dtype, device=device)<br>        timestep = torch.tensor(t_i, dtype=Y_t.dtype, device=device).repeat(batch_size)<br>        <span class="hljs-comment"># print(text_ids.shape, latent_image_ids.shape)</span><br>        <span class="hljs-comment"># get the unconditional vector field</span><br>        u_t_i = <span class="hljs-variable language_">self</span>.transformer(<br>            hidden_states=Y_t,<br>            timestep=timestep,<br>            guidance=guidance,<br>            pooled_projections=pooled_prompt_embeds,<br>            encoder_hidden_states=prompt_embeds,<br>            txt_ids=text_ids,<br>            img_ids=latent_image_ids,<br>            joint_attention_kwargs=<span class="hljs-variable language_">self</span>.joint_attention_kwargs,<br>            return_dict=<span class="hljs-literal">False</span>,<br>        )[<span class="hljs-number">0</span>]<br><br>        <span class="hljs-comment"># get the conditional vector field</span><br>        u_t_i_cond = (y_1 - Y_t) / (<span class="hljs-number">1</span> - t_i)<br><br>        <span class="hljs-comment"># controlled vector field</span><br>        <span class="hljs-comment"># Eq 8 dY_t = [u_t(Y_t) + γ(u_t(Y_t|y_1) - u_t(Y_t))]dt</span><br>        u_hat_t_i = u_t_i + gamma * (u_t_i_cond - u_t_i)<br>        Y_t = Y_t + u_hat_t_i * (sigmas[i] - sigmas[i + <span class="hljs-number">1</span>])<br>        progress_bar.update()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 推理pipeline.call：</span><br><span class="hljs-keyword">if</span> do_rf_inversion:<br>    v_t = -noise_pred<br>    v_t_cond = (y_0 - latents) / (<span class="hljs-number">1</span> - t_i)<br>    eta_t = eta <span class="hljs-keyword">if</span> start_timestep &lt;= i &lt; stop_timestep <span class="hljs-keyword">else</span> <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">if</span> decay_eta:<br>        eta_t = eta_t * (<span class="hljs-number">1</span> - i / num_inference_steps) ** eta_decay_power  <span class="hljs-comment"># Decay eta over the loop</span><br>    v_hat_t = v_t + eta_t * (v_t_cond - v_t)<br><br>    <span class="hljs-comment"># SDE Eq: 17 from https://arxiv.org/pdf/2410.10792</span><br>    latents = latents + v_hat_t * (sigmas[i] - sigmas[i + <span class="hljs-number">1</span>])<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-comment"># compute the previous noisy sample x_t -&gt; x_t-1</span><br>    latents = <span class="hljs-variable language_">self</span>.scheduler.step(noise_pred, t, latents, return_dict=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><p><img src="/../imgs/Paper-Reading_Inversion_based_%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/image-20250411200714985.png" alt="image-20250411200714985"></p></li></ol><h3 id="1-2-RF-Edit-Taming-Rectified-Flow-for-Inversion-and-Editing——腾讯"><a href="#1-2-RF-Edit-Taming-Rectified-Flow-for-Inversion-and-Editing——腾讯" class="headerlink" title="1.2 RF-Edit: Taming Rectified Flow for Inversion and Editing——腾讯"></a>1.2 RF-Edit: Taming Rectified Flow for Inversion and Editing——腾讯</h3><ol><li>ODE  泰勒展开 + 一阶导近似</li><li>为增强细节保留在Edit时利用了<strong>注意力共享</strong>: Inversion + 替换V【单流自注意力中的V_ref】</li></ol><p><img src="/../imgs/Paper-Reading_FlowEdit/image-20250410195502731.png" alt="image-20250410195502731"></p><p><img src="/../imgs/Paper-Reading_FlowEdit/Picture3.jpg" alt="Picture3.jpg"></p><h3 id="1-3-FlowEdit：-Inversion-Free-Text-Based-Editing-Using-Pre-Trained-Flow-Models-2024-12"><a href="#1-3-FlowEdit：-Inversion-Free-Text-Based-Editing-Using-Pre-Trained-Flow-Models-2024-12" class="headerlink" title="1.3  FlowEdit： Inversion-Free Text-Based Editing Using Pre-Trained Flow Models 2024.12"></a>1.3  FlowEdit： Inversion-Free Text-Based Editing Using Pre-Trained Flow Models 2024.12</h3><ol><li><p>理论创新：</p><ul><li>没去实现Inversion找中间高斯噪声的步骤，而是直接从源分布到目标分布【近似noise_free】，并在实际操作时根据n_avg取平均;</li><li>Noise_free的路径：分布转换的路径消耗更小，会更好；</li></ul></li><li><p>耗时依旧：但由于实现过程中的每一step都需要使用FLUX模型推理两次（v_tar、v_src），实现起来<strong>依然需要 2*steps!</strong>，与inversion方法相比并没有实质性的耗时优化；</p></li><li><p>代码：<strong>image_ids复制了src的image_ids</strong></p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">latent_tar_image_ids = latent_src_image_ids： 导致如果n_min=<span class="hljs-number">0</span>会存在ref原图的伪影<br></code></pre></td></tr></table></figure><ol start="4"><li>inversion vs FlowEdit 路径对比以及FlowEdit伪代码：</li></ol><p><img src="/../imgs/Paper-Reading_FlowEdit/image-20250408214137099.png" alt="image-20250408214137099"></p><ol start="5"><li><p>测试：</p><ul><li><p>n_min：伪影解决：timestep &#x3D; 28,  n_max &#x3D; 24, n_min &#x3D;0, n_avg&#x3D;1:  n_min设置为较小的数，能够在后期去噪阶段进行正常flow去噪，把伪影去掉；</p><blockquote><p>timestep 是去噪的全部步数，去噪时倒着数：从28,27，一直到1；</p><p>原论文代码中设置：</p><ul><li>timestep–n_max：不进行处理</li><li>n_max–n_min：flow_edit处理</li><li>n_min–1：正常flow去噪处理</li></ul></blockquote><p><img src="/../imgs/Paper-Reading_Inversion_based_%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/1744703713461-8.png" alt="img"></p></li><li><p>n_max：n_max的设置见SDEdit论文，调整了编辑的强度，n_max越大，编辑强度&#x2F;程度越大</p><p><img src="/../imgs/Paper-Reading_Inversion_based_%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/image-20250416105210796.png" alt="image-20250416105210796"></p></li></ul></li></ol><h2 id="2-Inversion-based-editing"><a href="#2-Inversion-based-editing" class="headerlink" title="2. Inversion_based_editing:"></a>2. Inversion_based_editing:</h2><h3 id="2-1-RF-Edit-inversion-替换单流自注意力中的-V-上文讲过"><a href="#2-1-RF-Edit-inversion-替换单流自注意力中的-V-上文讲过" class="headerlink" title="2.1 RF-Edit: inversion+ 替换单流自注意力中的 V (上文讲过)"></a>2.1 RF-Edit: inversion+ 替换单流自注意力中的 V (上文讲过)</h3><ol><li>ODE  泰勒展开 + 一阶导近似</li><li>为增强细节保留在Edit时利用了<strong>注意力共享</strong>: Inversion + <strong>替换V【单流自注意力中的V_ref】</strong></li></ol><h3 id="2-2-KV-Edit-inversion-拼接-K-V——背景保留能力-2025-3"><a href="#2-2-KV-Edit-inversion-拼接-K-V——背景保留能力-2025-3" class="headerlink" title="2.2 KV Edit: inversion + 拼接 K &amp; V——背景保留能力 2025.3"></a>2.2 KV Edit: inversion + 拼接 K &amp; V——背景保留能力 2025.3</h3><p>论文：</p><ol><li><p>如何实现inversion？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">类似于RF-Inversion，没有使用condition control<br></code></pre></td></tr></table></figure></li><li><p>反演步骤：包括前景和背景的分离反演，耗时*2？？？:</p><ul><li>前景反演：获得前景对应的高斯分布噪声分布，以作为去噪的初始表示</li><li>背景反演：获得反演过程中的<strong>cached-K_bg &#x2F; cached_V_bg，以指导去噪的背景保留</strong></li></ul></li><li><p>去噪步骤：</p><ul><li>初始表示：除了removal任务使用reint（加入随机噪声）外，其他均使用反演步骤得到的前景反演噪声XN_fg</li><li><strong>双流交叉注意力中：拼接来自背景的K&#x2F;V</strong>？具体如何实现</li></ul></li></ol><p>针对object removal：</p><ul><li>re-init : <code>after inversion, we replace ztN with fused noise z′tN = noise·tN +ztN ·(1−tN )</code>disrupt the original content information.</li><li><code>incorporate an attention mask during the inversion process</code></li></ul><p><img src="/../imgs/Paper-Reading_Inversion_based_%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/image-20250409152124969.png" alt="image-20250409152124969"></p><p><img src="/../imgs/Paper-Reading_Inversion_based_%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/image-20250411221055432.png" alt="image-20250411221055432"></p><p>有些不明白的描述——看代码：</p><p><code>Q_fg represents queries containing only foreground tokens</code></p><p><code>(Kfg, Kbg) and (Vfg, Vbg) denote the concatenation of background and foreground keys and values in their proper order (equivalent to the complete image’s keys and values),</code></p><p>crop  fg操作？？？<code>performing cropping at both input and output of the attention layer,</code></p><p>提出改进：是否可以更近一步K V不使用contencate，而是替换？：personalize anything是对图像token_ref替换，同时对位置编码也做了操作</p><h3 id="2-3-Personalize-Anything-inversion-图像序列替换——inpaint、outpaint、个性化保持-2025-3"><a href="#2-3-Personalize-Anything-inversion-图像序列替换——inpaint、outpaint、个性化保持-2025-3" class="headerlink" title="2.3 Personalize Anything: inversion + 图像序列替换——inpaint、outpaint、个性化保持 2025.3"></a>2.3 Personalize Anything: inversion + 图像序列替换——inpaint、outpaint、个性化保持 2025.3</h3><ol><li>如何实现inversion：RF-Inversion【代码】，RF-Edit【论文】</li><li>反演步骤：<ul><li>获得cached_tokens【无位置ids信息】:只获得语义token，不要位置</li></ul></li><li>去噪步骤：<ul><li>无位置ids信息的 cached_tokens <strong>替换replace</strong> position_encoded denoising tokens：无伪影，对ref关注，利于重建ref</li><li>替换的具体实现：依赖于mask：X&#96;&#x3D; X ⊙ (1 − M) + X_ref ⊙ M，其中M可以经过任意<strong>平移</strong>；</li><li><strong>timestep-adaptive token replacement</strong>：<ul><li>在去噪的前期阶段执行<strong>替换</strong></li><li>在去噪的后期阶段：<strong>拼接</strong> 【denoising_tokens，零ids置为的ref_tokens，text_tokens】执行MMA多模态注意力：增强语义能力</li></ul></li><li><strong>Patch Perturbation for Variation</strong>：减少过拟合<ul><li>3*3window 打乱ids</li><li>Mask的增强：膨胀 腐蚀</li></ul></li></ul></li><li>消融实验：<ul><li>position_encoded denoising tokens 与 position_encoded cached_tokens <strong>拼接</strong>：两者的ids都保持：产生ref伪影——DiT对ids位置很敏感</li><li>position_encoded denoising tokens 与 全部ids置为(0,0)或者平移后的（i+w, j） 的 cached_tokens <strong>拼接</strong>：对ref不产生关注</li></ul></li><li>多任务能力：<ul><li>Mask的灵活移动性带来了：布局引导</li><li>多主体生成</li><li>图像编辑：disable perturbations and set τ to 10% total steps：inpaint、outpaint</li></ul></li></ol><p><img src="/../imgs/Paper-Reading_Inversion_based_%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/image-20250412115003441.png" alt="image-20250412115003441"></p><ol start="6"><li><p>测试：</p><p>缺：<strong>ref_token的替换并不能十分严谨的重建&#x2F;保持主体特征，而且inpaint内外不连贯</strong>，需要依赖于tau阈值参数的调整，</p><p><img src="/../imgs/Paper-Reading_Inversion_based_%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/image-20250414223622758.png" alt="image-20250414223622758"></p></li><li><p>代码：</p><ul><li><p>RFInversionParallelFluxPipeline：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pipeline_flux_rf_inversion:与RF-Inversion相同</span><br>    inverted_latents, image_latents, latent_image_ids = pipe.invert( <br>        source_prompt=<span class="hljs-string">&quot;&quot;</span>,  <span class="hljs-comment"># 使用空描述</span><br>        image=init_image, <br>        height=height,<br>        width=width,<br>        num_inversion_steps=timestep, <br>        gamma=<span class="hljs-number">1.0</span>)<br><span class="hljs-comment"># pipe.forward():执行ref_tokens替换的图像编辑生成：# 新增参数inverted_latents、start_timestep、stop_timestep： 去噪过程中timestep从1-0逐步减小！</span><br><br>latents = inverted_latents<br>new_latents, _ = <span class="hljs-variable language_">self</span>.prepare_latents(<span class="hljs-number">1</span>,..., generator,latents=<span class="hljs-literal">None</span>,)<br>latents = torch.cat((latents, new_latents), dim=<span class="hljs-number">0</span>) <span class="hljs-comment"># [0]:inverted_noise_latent, [1]:rand_noise_latent</span><br></code></pre></td></tr></table></figure></li><li><p>set_flux_transformer_attn_processor：定义attn processor，每次不同的任务都需要重新定义</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># img_dims: 图像latent序列长度，VAE8倍压缩比 = pipe.vae_scale_factor=8， patchify图块化2*2的两倍压缩比, 总压缩比16</span><br>    set_flux_transformer_attn_processor(<br>        pipe.transformer,<br>        set_attn_proc_func=<span class="hljs-keyword">lambda</span> name, dh, nh, ap: PersonalizeAnythingAttnProcessor( name=name, tau=tau/<span class="hljs-number">100</span>, mask=mask, shift_mask=shift_mask, device=device, img_dims=img_dims, concept_process=<span class="hljs-literal">False</span>),) <br><span class="hljs-comment"># 通过每次重新定义attn processor来实现mask、tau阈值、shift_mask、concept_process等参数传递：shift_mask、mask</span><br></code></pre></td></tr></table></figure></li><li><p>PersonalizeAnythingAttnProcessor、MultiPersonalizeAnythingAttnProcessor 对比原始 FluxAttnProcessor2_0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># FluxAttnProcessor2_0：在q k v 获得后，attention计算之前，添加旋转的位置编码：</span><br>    <span class="hljs-keyword">if</span> image_rotary_emb <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">from</span> .embeddings <span class="hljs-keyword">import</span> apply_rotary_emb<br><br>        query = apply_rotary_emb(query, image_rotary_emb)<br>        key = apply_rotary_emb(key, image_rotary_emb)<br><br><span class="hljs-comment"># PersonalizeAnythingAttnProcessor.forward()：多了timestep参数:由self._joint_attention_kwargs[&quot;timestep&quot;] =进行传递</span><br>    <span class="hljs-comment"># concept_feature_    r_hidden_states？？？？</span><br>    <span class="hljs-keyword">if</span> encoder_hidden_states <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        concept_feature_ = hidden_states[<span class="hljs-number">0</span>, <span class="hljs-variable language_">self</span>.mask, :]  <span class="hljs-comment">#  self.mask = mask.view(img_dims).bool().to(device)</span><br>    <span class="hljs-keyword">else</span>:<br>        concept_feature_ = hidden_states[<span class="hljs-number">0</span>, <span class="hljs-number">512</span>:, :][<span class="hljs-variable language_">self</span>.mask, :] <span class="hljs-comment"># flux使用512个text_tokens</span><br><br>    <span class="hljs-keyword">if</span> r_k <span class="hljs-keyword">or</span> r_q <span class="hljs-keyword">or</span> r_v:<br>        r_hidden_states = hidden_states<br>        <span class="hljs-keyword">if</span> encoder_hidden_states <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            r_hidden_states[<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.shift_mask, :] = concept_feature_<br>    <span class="hljs-keyword">else</span>:<br>        text_hidden_states = hidden_states[<span class="hljs-number">1</span>, :<span class="hljs-number">512</span>, :]<br>        image_hidden_states = hidden_states[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>:, :]<br>        image_hidden_states[<span class="hljs-variable language_">self</span>.shift_mask, :] = concept_feature_<br><br>        r_hidden_states[<span class="hljs-number">1</span>] = torch.cat([text_hidden_states, image_hidden_states], dim=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure></li></ul></li></ol><p>insight: 视频大模型具有对物理规律的理解，光照光影位置关系等，可以利用视频大模型的先验进行图片领域上的任务，如图像编辑、多视图生成</p><p>style-align</p><p>Training-Free Consistent Text-to-Image Generation</p><p><img src="/../imgs/Paper-Reading_Inversion_based_%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/image-20250411170316940.png" alt="image-20250411170316940"></p>]]></content>
    
    
    <categories>
      
      <category>Paper Reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DiT</tag>
      
      <tag>Flow_based</tag>
      
      <tag>图像编辑</tag>
      
      <tag>Inversion</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flow-based基于流的图像生成</title>
    <link href="/Paper-Reading/Flow_based%E7%94%9F%E6%88%90/"/>
    <url>/Paper-Reading/Flow_based%E7%94%9F%E6%88%90/</url>
    
    <content type="html"><![CDATA[<p>多模态理解与生成 分类：</p><ul><li>基于扩散的：都是估计一种分布转换<ul><li>基于扩散Diffusion:  ϵ -Prediction，UNet、DiT、MM-DiT；估计噪声&#x2F;数据分布，不限定&#x3D;&#x3D;&gt;也就是走曲线的概率密度转换路径：<strong>缺：训练时间长采样慢</strong><ul><li>前行过程：通过不可学习的Schedule 对样本进行加噪，多次加噪变换为正态分布</li><li>反向过程：从正态分布采样，并通过模型隐式的学习反向过程的噪声，去噪得到生成的样本【迭代多步生成】</li></ul></li><li>基于流Flow： v -Prediction，DiT、MM-DiT；估计向量场&#x2F;速度场，尽量走直线的概率密度转换路径，与扩散落实在代码上的区别是“训练目标函数和采样函数”<ul><li>前行过程：通过显式的可学习的可逆变换将样本分布变换为正态分布</li><li>反向过程：从正态分布采样，并通过上述变换的逆变换得到生成的样本【近似直线一步生成】</li></ul></li></ul></li><li>基于自回归的：各种AR的序列预测顺序</li><li>基于扩散+自回归的</li></ul><p>基础框架：</p><ul><li>Transformer：decoder-only transformer？</li><li>Diffusion-Transformer（DiT）</li><li>MM-DiT</li></ul><p><img src="/../imgs/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90/image-20250408144423138.png" alt="image-20250408144423138"></p><h3 id="2-Flow-based-Method-原理理解："><a href="#2-Flow-based-Method-原理理解：" class="headerlink" title="2. Flow-based Method 原理理解："></a>2. Flow-based Method 原理理解：</h3><p>阅读博文：</p><ul><li>Rectified Flow论文原作者的中文解读：<a href="https://zhuanlan.zhihu.com/p/603740431">ICLR2023] 扩散生成模型新方法：极度简化，一步生成</a></li><li><a href="https://littlenyima.github.io/posts/51-flow-matching-for-diffusion-models/index.html">Flow Matching 理论详解</a></li><li><a href="https://zhuanlan.zhihu.com/p/11686643707">深入理解Rectified Flow，完善统一扩散框架</a></li></ul><blockquote><p>flow: 使用时间参数化的向量场来定义一个流函数 <strong>Flow</strong>，从而在连续时间内对概率密度进行变换</p><p>marginal distribution: 边际分布？</p><p>可以把Rectified Flow理解为<strong>保持线性插值边缘分布相等的，具有因果的ODE</strong></p></blockquote><p>常见的Flow-based包括：</p><ul><li>Normalizing Flow：离散可逆变换，对概率分布进行建模，实现从先验分布（即，高斯分布）到目标分布的可逆转换过程并采样生成；</li><li>Continuous Normalizing Flow（CNF）：连续可逆变换【ODE】；<ul><li>Rectified Flow（linear flows）2022.10：<strong>一步沿着直线采样生成【ODE】</strong>：Domain Transfer，两次训练</li><li>Flow Matching（FM）、Conditional Flow Matching（CFM）2022.10：【ODE】：几乎同上</li></ul></li></ul><h4 id="2-1-Rectified-flow-2022-10"><a href="#2-1-Rectified-flow-2022-10" class="headerlink" title="2.1 Rectified flow 2022.10:"></a>2.1 Rectified flow 2022.10:</h4><p>Rectified flow + 【蒸馏 与 Reflow回流的区别】</p><p><img src="/../imgs/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90/image-20250408143435584.png" alt="Rectified Flow"></p><p><img src="/../imgs/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90/image-20250408144230797.png" alt="image-20250408144230797"></p><h4 id="2-2-Flow-Matching-–-Conditional-FM-2022-10"><a href="#2-2-Flow-Matching-–-Conditional-FM-2022-10" class="headerlink" title="2.2 Flow Matching –&gt; Conditional FM 2022.10:"></a>2.2 Flow Matching –&gt; Conditional FM 2022.10:</h4><p>先验分布设定为高斯分布，包括μ_t均值函数、sigma_t标准差函数的不同，可以构建不同的高斯条件概率路径：</p><ul><li><strong>Variance Preserving，VP</strong> 方差保持扩散路径：传输路径轨迹为曲线</li><li><strong>Variance Exploding，VE</strong> 方差爆炸扩散路径：传输路径轨迹为曲线</li><li><strong>最优传输（Optimal Transport，OT）</strong> 路径（CFM）：传输路径轨迹为直线，得到更快的训练速度和生成速度、更好的性能表现</li></ul><p><img src="/../imgs/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90/image-20250408152543388.png" alt="image-20250408152543388"><img src="/../imgs/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90/image-20250408151908334.png" alt="image-20250408151908334"></p><p><img src="/../imgs/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90/image-20250408150909885.png" alt="image-20250408150909885"><img src="/../imgs/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90/image-20250408151159712.png" alt="image-20250408151159712"></p><h3 id="3-Flow-based-Model-Transformer"><a href="#3-Flow-based-Model-Transformer" class="headerlink" title="3. Flow_based Model + Transformer:"></a>3. Flow_based Model + Transformer:</h3><h4 id="3-1-Transformer-原理探究：DiT"><a href="#3-1-Transformer-原理探究：DiT" class="headerlink" title="3.1 Transformer 原理探究：DiT"></a>3.1 Transformer 原理探究：DiT</h4><p>博文阅读：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/684125968">Diffusion Transformer（DiT）原理与源码解析 - 知乎</a></li></ul><p>ViT &#x3D; Transformer用在CV领域：将图像进行 patchify 得到固定大小的 patch 序列，作为输入馈送进标准 Transformer Encoder，然后进行下游任务的处理</p><p>DiT &#x3D; 调整Normalization后的ViT + LDM（输入是latent）： </p><ul><li>TimestepEmbedder:  timestep_embedding(对数正弦位置编码，log_SPE) + MLP结构</li><li>LabelEmbedder: dropout丢弃实现CFG能力</li><li>DiTBlock: SelfAttention、FeedForward、Normalization: DiT 使用 adaln_zero 来代替标准 transformer 中的 Layer Normalization</li><li>Final Layer：</li><li>Patchify： timm.PatchEmbed（）:2*2， SPE位置编码</li></ul><p><img src="/../imgs/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90/image-20250408164934759.png" alt="ViT 到 DiT"></p><h4 id="3-2-SD3-–-Rectified-Flow-stability-ai-2024-3：Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis"><a href="#3-2-SD3-–-Rectified-Flow-stability-ai-2024-3：Scaling-Rectified-Flow-Transformers-for-High-Resolution-Image-Synthesis" class="headerlink" title="3.2 SD3 –&gt; [Rectified Flow] _stability.ai 2024.3：Scaling Rectified Flow Transformers for High-Resolution Image Synthesis"></a>3.2 SD3 –&gt; [Rectified Flow] _stability.ai 2024.3：Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</h4><p><strong>主要贡献</strong>：<br><strong>1）改进的<a href="https://zhida.zhihu.com/search?content_id=240459073&content_type=Article&match_order=1&q=%E5%99%AA%E5%A3%B0%E9%87%87%E6%A0%B7%E6%8A%80%E6%9C%AF&zhida_source=entity">噪声采样技术</a>（Improved Noise Sampling Techniques）</strong>：</p><ul><li>作者针对修正流模型（Rectified Flow Models）提出了一种新的噪声采样方法（<strong>Timestep Samplers</strong>），这种方法通过偏向感知相关尺度（perceptually relevant scales）来提高训练效率和生成图像的质量。这种改进有助于模型在训练过程中更有效地学习数据到噪声的转换过程。</li><li>introducing a re-weighting of the noise scales in rectified flow models：SNR采样器</li><li>提到了一个diffusion的scheduler缺陷：Common Diffusion Noise Schedules and Sample Steps are Flawed</li></ul><p><strong>2）MM-DiT：novel transformer-based architecture</strong> 文图双向流</p><p><img src="/../imgs/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90/image-20250408163912375.png" alt="image-20250408163912375"></p><h4 id="3-3-Flux-–-Flow-Matching-Black-Forest-Labs-没公开论文"><a href="#3-3-Flux-–-Flow-Matching-Black-Forest-Labs-没公开论文" class="headerlink" title="3.3 Flux –&gt; [Flow Matching] _Black Forest Labs 没公开论文"></a>3.3 Flux –&gt; [Flow Matching] _Black Forest Labs 没公开论文</h4><p>博文阅读：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/741939590">FLUX.1 原理与源码解析 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/714150390">FLUX.1 源码深度前瞻解读</a></li></ul><p>主要贡献</p><ul><li>DoubleStreamBlock 双流块 （与MM-DiT几乎相同）</li><li>SingleStreamBlock 单流块 （与DiT几乎相同）</li></ul><p><img src="/../imgs/Flow_based%E7%94%9F%E6%88%90/v2-ef89636e6f9d4c333116cdba283f1151_r.png" alt="FLUX"></p><h3 id="4-Transformer-位置编码"><a href="#4-Transformer-位置编码" class="headerlink" title="4. Transformer 位置编码"></a>4. Transformer 位置编码</h3><p>阅读博文：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/631363482">Transformer位置编码（基础）</a></li><li><a href="https://zhuanlan.zhihu.com/p/454482273">Transformer学习笔记一：Positional Encoding（位置编码）</a></li></ul><p>Positional Encoding：就是将位置信息添加（嵌入）到Embedding词向量中，让Transformer保留词向量的<strong>位置信息</strong>，可以提高模型对序列的理解能力；</p><ol><li>为每个字输出<strong>唯一的编码</strong>；</li><li>不同长度的句子之间，任何两个字之间的差值应该保持一致？</li><li>编码值应该是<strong>有界</strong>的。</li></ol><p>位置编码分类：</p><ul><li><p>绝对位置编码 <code>Absolute Position Embedding</code>：</p><ul><li><p><strong>学习式位置编码（Learned Positional Embedding）</strong>：方法是最普遍的绝对位置编码方法，该方法直接对不同的位置随机初始化一个 <code>postion embedding</code>，加到 <code>word embedding</code> 上输入模型，<strong>作为参数进行训练</strong>；举例：<a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1810.04805.pdf">BERT</a>、<a href="https://link.zhihu.com/?target=https://ailab-ua.github.io/courses/resources/GPT-2_Radford_2018.pdf">GPT-2</a></p></li><li><p><strong>正弦位置编码（Sinusoidal Positional Encoding, SPE）</strong>：是通过将正弦和余弦函数的不同频率应用于输入序列的位置来计算位置编码；</p><p><img src="/../imgs/Flow_based%E7%94%9F%E6%88%90/image-20250408180932773.png" alt="SPE正弦位置编码"></p></li></ul></li><li><p><strong>相对位置编码 <code>Relative Position Embedding</code></strong>：</p><ul><li>Learned Positional Encoding：通过学习一组可学习参数来计算位置编码- </li><li><strong>旋转位置编码RoPE</strong></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>Paper Reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>多模态</tag>
      
      <tag>flow_based</tag>
      
      <tag>flux sd3</tag>
      
      <tag>DiT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AutoRegressive自回归图像生成</title>
    <link href="/Paper-Reading/AutoRegressive%E8%87%AA%E5%9B%9E%E5%BD%92/"/>
    <url>/Paper-Reading/AutoRegressive%E8%87%AA%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<p>自回归模型（AR）：通过对序列中先前的输入进行测量来自动预测序列中的下一个分量，是一种用于时间序列分析的统计技术，它假设时间序列的当前值是其过去值的函数。自回归模型使用类似的数学技术来确定序列中元素之间的概率相关性。然后，它们使用所得知识来猜测未知序列中的下一个元素。<br>视觉自回归模型（VAR）：</p><p>自回归图像生成技术探索路线：VQ-GAN、ImageGPT、CogView、JetFormer:</p><blockquote><p><a href="https://www.cnblogs.com/LexLuc/p/18798718">ChatGPT-4o 更新生图能力：原生多模态的图文生图技术详解：自回归路线的逆袭 </a></p><p>Decoder-Only Transformer 模型？</p></blockquote><h2 id="1-AR自回归图像生成：AE（Visual-Tokenizers）"><a href="#1-AR自回归图像生成：AE（Visual-Tokenizers）" class="headerlink" title="1. AR自回归图像生成：AE（Visual Tokenizers）"></a>1. AR自回归图像生成：AE（Visual Tokenizers）</h2><p>阅读博文：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/22553430415">自回归图像生成中的Visual Tokenizers: AEs</a></li><li><a href="https://zhuanlan.zhihu.com/p/681895334">AutoEncoder与自回归图像生成</a></li></ul><p>AE与AR图像生成的关系：第一阶段训练 “AEs”，第二阶段对AR自回归图像token预测模型进行训练（如<a href="https://zhida.zhihu.com/search?content_id=239662218&content_type=Article&match_order=1&q=PixelCNN&zhida_source=entity">PixelCNN</a>、Transformer）；</p><p>经典AE技术：</p><ul><li><p><strong>AE</strong>：图片经过编码器得到 latent code， 解码得到重建图片；缺：对噪声敏感</p></li><li><p><strong>VAE（变分自编码）</strong>：通过编码器学习出mean和std编码，随机采样一个<strong>正态分布</strong>的编码α，通过 α*std+mean 重采样得到latent code【确保梯度可传播】，解码器进行重建；优：对输入噪声不敏感，预先知道每个属性都是服从正态分布，对于任意采样都能重构出鲁棒的图片；</p></li><li><p><strong>VQ-VAE（vector quantized variational autoencoder）</strong>：通过编码器学习出中间编码，然后通过最邻近搜索将中间编码映射为<a href="https://zhida.zhihu.com/search?content_id=174419408&content_type=Article&match_order=1&q=codebook&zhida_source=entity">codebook</a>中K个向量之一【<strong>离散量化+查表</strong>】，通过解码器对latent code进行重建。生成模式是“自回归的 **pixcl-CNN **+codebook”， 交叉熵损失？</p><p><img src="/../imgs/AutoRegressive%E8%87%AA%E5%9B%9E%E5%BD%92/image-20250407190626106.png" alt="AE&#x2F;VAE&#x2F;VQVAE"></p><p><img src="/../imgs/AutoRegressive%E8%87%AA%E5%9B%9E%E5%BD%92/v2-8b094c3f7c0f105316651602a425a850_1440w.jpg" alt="VA-VAE：PIXEL-CNN"></p></li><li><p>VQ-VAE 2：多层次的VQ-VAE：层次化的向量量化</p></li><li><p>VQ-GAN：引入Transformer?</p></li><li><p><strong>MaskGIT(MAE)</strong>：</p></li><li><p><strong>DALL·E tokenizer 【D-VAE】</strong>:<a href="https://sunlin-ai.github.io/2022/06/02/DALL-E.html">D-VAE</a>：生成模式是”自回归的 <strong>GPT</strong>+codebook“</p></li></ul><h2 id="2-AR自回归图像生成：分类"><a href="#2-AR自回归图像生成：分类" class="headerlink" title="2. AR自回归图像生成：分类"></a>2. AR自回归图像生成：分类</h2><p>阅读博文：</p><ul><li><p><a href="https://blog.csdn.net/v_JULY_v/article/details/136318383?login=from_csdn">文生图中从扩散模型到流匹配的演变：从SDXL到Stable Diffusion3(含Flow Matching和Rectified Flow的详解)</a>、</p></li><li><p><a href="https://blog.csdn.net/v_JULY_v/article/details/131205615">AI绘画原理解析：从CLIP、BLIP到DALLE、DALLE 2、DALLE 3、Stable Diffusion</a></p></li></ul><h4 id="2-1-按照预测序列顺序进行的分类："><a href="#2-1-按照预测序列顺序进行的分类：" class="headerlink" title="2.1 按照预测序列顺序进行的分类："></a>2.1 按照预测序列顺序进行的分类：</h4><p>自回归图像生成需要设定一种比较合理的<em>序列生成顺序</em>，以及定义合适的序列里面的每一个“元素”，据此，可以将AR技术分类：</p><ul><li><p>next-pixel-prediction: 【raster-order、random-order】，AR、MAR，raster-order：GPT系列+单向Transformer，random-order：BERT系列+双向Transformer</p></li><li><p>next-patch-prediction: 【raster-order】，PAR</p></li><li><p>Next-Block-Prediction</p></li><li><p>next-set-token-prediction: 【raster-order、random-order、曼哈顿距离order】， 如Muse、MaskGit、NAR、ZipAR</p></li><li><p>Next-Scale-Prediction:  VAR [北大 字节]</p></li><li><p>Next-Frequency-Prediction: FAR</p></li></ul><blockquote><p>raster-order光栅顺序：从左到右、从上到下，从左上角到右下角；</p></blockquote><p><img src="/../imgs/AutoRegressive%E8%87%AA%E5%9B%9E%E5%BD%92/v2-f7368634982ddee73d837d5cb12ccaed_1440w.jpg" alt="img"></p><h4 id="2-2-按照-image-token的离散、连续进行分类："><a href="#2-2-按照-image-token的离散、连续进行分类：" class="headerlink" title="2.2 按照 image token的离散、连续进行分类："></a>2.2 按照 image token的离散、连续进行分类：</h4><blockquote><p>阅读博文：《Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens》Google DeepMind</p></blockquote><p>输入的分辨率为 256×256 的图片：</p><ul><li><p>离散Image Tokenizer使用 VQGAN 模型，将图像编码为 16×16 个离散Token，Codebook大小为 8192</p></li><li><p>连续Image Tokenizer采用来自 Stable Diffusion 的一种，将图像编码为连续的 32×32 个Token，每个token含 4 个通道，为与离散token器序列长度一致，进一步将每 2×2 个连续token组合为一个token，最终序列长度为 256，每个token含 16 个通道，连续Token重建质量显著高于离散Token</p></li></ul><h2 id="3-AR自回归-Diffusion扩散模型相结合：多模态理解与生产【终极形态？】"><a href="#3-AR自回归-Diffusion扩散模型相结合：多模态理解与生产【终极形态？】" class="headerlink" title="3. AR自回归+Diffusion扩散模型相结合：多模态理解与生产【终极形态？】"></a>3. AR自回归+Diffusion扩散模型相结合：多模态理解与生产【终极形态？】</h2><p>阅读博文：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/3442569070">自回归+扩散模型的图像生成模型</a></li></ul><p>自然语言理解、生成：自回归 + Transformer，扩散较少</p><p>图片理解、生成：视觉自回归、扩散 + Transformer【MM-DiT】</p><p>举例：</p><ul><li><p>TransFusion：</p><p><img src="/../imgs/AutoRegressive%E8%87%AA%E5%9B%9E%E5%BD%92/image-20250408101804847.png" alt="image-20250408101804847"></p></li></ul><h2 id="4-经典AR视觉生成举例："><a href="#4-经典AR视觉生成举例：" class="headerlink" title="4. 经典AR视觉生成举例："></a>4. 经典AR视觉生成举例：</h2><p>阅读博文：<a href="https://zhuanlan.zhihu.com/p/7219374450">自回归图像生成技术举例</a></p><h4 id="4-1-ImageGPT-Generative-Pretraining-from-Pixels-自回归视觉模型的先驱-OpenAI-2020-6"><a href="#4-1-ImageGPT-Generative-Pretraining-from-Pixels-自回归视觉模型的先驱-OpenAI-2020-6" class="headerlink" title="4.1 ImageGPT: Generative Pretraining from Pixels 自回归视觉模型的先驱- OpenAI 2020.6"></a>4.1 ImageGPT: Generative Pretraining from Pixels 自回归视觉模型的先驱- OpenAI 2020.6</h4><p>GPT&#x3D;Generative Pre-trained Transformer；ImageGPT会逐个预测像素序列中的每一个像素值。这意味着，在预测某一个像素值时，模型只能依据已经预测出的像素值进行推断。这种自回归的方式使得ImageGPT能够学习到图像中像素之间的依赖关系，从而生成更加自然、连贯的图像。</p><p>阅读博文<a href="https://zhuanlan.zhihu.com/p/352350329">iGPT详解</a></p><p>网络结构：GPT-2，无监督训练，对概率密度进行自回归建模</p><p>能力：图片补全、分类等下游任务（图像理解）</p><p>缺：降采样到32&#x2F;48&#x2F;64分辨率后K-means聚类（k&#x3D;512）</p><p><img src="/../imgs/AutoRegressive%E8%87%AA%E5%9B%9E%E5%BD%92/v2-dfb18172e3ed94a26ebe892334f179ed_1440w.jpg" alt="iGPT图像缩减和展开，预训练，微调和线性探测"></p><h4 id="4-2-DALLE-系列-DALLE1自回归，-DALLE2扩散，DALLE3不清楚"><a href="#4-2-DALLE-系列-DALLE1自回归，-DALLE2扩散，DALLE3不清楚" class="headerlink" title="4.2 DALLE 系列 - DALLE1自回归， DALLE2扩散，DALLE3不清楚"></a>4.2 DALLE 系列 - DALLE1自回归， DALLE2扩散，DALLE3不清楚</h4><p>博文阅读: <a href="https://blog.csdn.net/weixin_44966641/article/details/136289437">DALL-E 系列 (1-3)</a></p><p><img src="/../imgs/AutoRegressive%E8%87%AA%E5%9B%9E%E5%BD%92/6b7e8268aca7807eda8d7bf0ab66c88a.png" alt="DALLE 1:[Zero-Shot Text-to-Image Generation](https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2102.12092)"></p><h4 id="4-3-Visual-AutoRegressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction-字节-北大-2024-6"><a href="#4-3-Visual-AutoRegressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction-字节-北大-2024-6" class="headerlink" title="4.3 Visual AutoRegressive Modeling: Scalable Image Generation via Next-Scale Prediction 字节 北大 2024.6"></a>4.3 Visual AutoRegressive Modeling: Scalable Image Generation via Next-Scale Prediction 字节 北大 2024.6</h4><ol><li>Visual AutoRegressive (VAR) transformers： 区别于以往的“next-token prediction”，本文提出了“next-scale prediction”或者“next-resolution prediction”；</li><li>架构：GPT-2类似的Transformer架构，视觉自回归，两阶段训练</li><li>符合两条经验：Scaling Laws；zero-shot generalization</li></ol><p><img src="/../imgs/AutoRegressive%E8%87%AA%E5%9B%9E%E5%BD%92/image-20250407163011096.png" alt="image-20250407163011096"></p><p><img src="/../imgs/AutoRegressive%E8%87%AA%E5%9B%9E%E5%BD%92/image-20250407164722640.png" alt="VAR两阶段训练"></p><p><img src="/../imgs/AutoRegressive%E8%87%AA%E5%9B%9E%E5%BD%92/v2-0ba6bc4de789f744e0c833df74158ddd_1440w.jpg" alt="VAR两阶段训练伪代码"></p><h4 id="4-4-JetFormer-谷歌-2024-11-【待看】"><a href="#4-4-JetFormer-谷歌-2024-11-【待看】" class="headerlink" title="4.4 JetFormer: 谷歌 2024.11 【待看】"></a>4.4 JetFormer: 谷歌 2024.11 【待看】</h4><p>transformer decoder-only models trained for next-token prediction</p><p>由 Transformer 和归一化流组成的生成模型，可以从头开始训练，以端到端的方式联合建模文本和原始像素</p><p>通过预训练的、冻结的 VQ-VAE 将高维图像像素空间压缩为低维离散 Token 序列，然后使用 Transformer 解码器对压缩序列进行建模</p><p><img src="/../imgs/AutoRegressive%E8%87%AA%E5%9B%9E%E5%BD%92/image-20250407205226990.png" alt="image-20250407205226990"></p><h4 id="4-5-RandAR-字节（待看）"><a href="#4-5-RandAR-字节（待看）" class="headerlink" title="4.5 RandAR 字节（待看）"></a>4.5 RandAR 字节（待看）</h4>]]></content>
    
    
    <categories>
      
      <category>Paper Reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AutoRegressive</tag>
      
      <tag>图像生成</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-Reading_Video_Generation</title>
    <link href="/Paper-Reading/Paper-Reading-Video-Generation/"/>
    <url>/Paper-Reading/Paper-Reading-Video-Generation/</url>
    
    <content type="html"><![CDATA[<h1 id="Diffusion-based-methods"><a href="#Diffusion-based-methods" class="headerlink" title="Diffusion based methods:"></a>Diffusion based methods:</h1><p>图生视频：</p><ol><li>文生<strong>图</strong>模型 + 时序建模：时序建模技术包括：“inter-frame attention”即temporal-layers，或者motion_module；</li><li>文生<strong>视频</strong>模型 + 图片特征：视觉特征提取（CLIP-image-encoder、Parallel-UNet-encoder &#x2F; ControlNet），latent初始化；</li></ol><ul><li>Animate-Anyone【图生视频】</li><li>Stable Video Diffusion(SVD)</li></ul><h2 id="1-Stable-Video-Diffusion：StabilityAI-2023-11"><a href="#1-Stable-Video-Diffusion：StabilityAI-2023-11" class="headerlink" title="1. Stable Video Diffusion：StabilityAI 2023.11"></a>1. Stable Video Diffusion：StabilityAI 2023.11</h2><h2 id="2-AnimateDiff：2023-11-略"><a href="#2-AnimateDiff：2023-11-略" class="headerlink" title="2. AnimateDiff：2023.11 略"></a>2. AnimateDiff：2023.11 略</h2><h2 id="3-Animate-Anyone：阿里巴巴-通义-2023-12"><a href="#3-Animate-Anyone：阿里巴巴-通义-2023-12" class="headerlink" title="3. Animate-Anyone：阿里巴巴-通义 2023.12"></a>3. Animate-Anyone：阿里巴巴-通义 2023.12</h2><ol><li><p>贡献点：</p><ul><li><p>ReferenceNet: 平行的UNet-encoder提取参考人物细节特征【优点：相比于CLIP-Image-encoer的输入分辨率只能224*224且强调语义，能够接受更大的分辨率输入且强调细粒度细节特征和语义，】</p></li><li><p>Pose Guider: 姿势控制器: 高斯权重、零投影初始化，加到noise_latent上</p></li><li><p>Temporal-Attention: 时序注意力层：确保帧间的空间、时间连续性，串联在两个注意力机制后：沿着 t 维度进行自注意力机制</p><p><code>Specifically, for a feature map x∈b×t×h×w×c, we first reshape it to x∈(b×h×w)×t×c, and then perform temporal attention, which refers to self-attention along the dimension t.</code></p></li></ul></li><li><p>训练策略：</p><ul><li>两阶段：<ul><li>先使用没有 Temporal-Attention 时序注意力层的网络训练Pose Guider：使用单帧；</li><li>再只训练 Temporal-Attention 模块，权重由 AnimateDiff 初始化：使用24帧；</li></ul></li><li>测试指标：SSIM， PSNR， LPIPS，FVD</li></ul></li><li><p>局限：</p><ul><li>手部运动失真、模糊</li><li>单视角参考：背面生成有不稳定</li></ul></li></ol><p><img src="/../imgs/Paper-Reading-Video-Generation/image-20250326100721485.png" alt="animate-anyone"></p><h1 id="Transformer-based-methods"><a href="#Transformer-based-methods" class="headerlink" title="Transformer based methods:"></a>Transformer based methods:</h1><ul><li>CogVideo</li></ul><p><img src="/../imgs/Paper-Reading-Video-Generation/640.webp" alt="图片"></p>]]></content>
    
    
    <categories>
      
      <category>Paper Reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>stable_video_diffusion</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Reserch-Insights_Stylized-generation-based-diffusion</title>
    <link href="/Research-Insights/Research-Insights_Stylized-generation-based-diffusion/"/>
    <url>/Research-Insights/Research-Insights_Stylized-generation-based-diffusion/</url>
    
    <content type="html"><![CDATA[<h1 id="Stylized-generation（Based-Diffusion）基于扩散模型的风格化生成"><a href="#Stylized-generation（Based-Diffusion）基于扩散模型的风格化生成" class="headerlink" title="Stylized generation（Based-Diffusion）基于扩散模型的风格化生成"></a>Stylized generation（Based-Diffusion）基于扩散模型的风格化生成</h1><p>面试时被问到关于风格化生成的发展路线，一下子紧张只说出了2个工作，但其实Diffusion发展至今已经有很多这方面的工作了。现在来做个总结，梳理一下风格迁移&#x2F;风格化生成的发展路线，加深一下理解。</p><blockquote><p>本文前提：只考虑由图像描述的风格提示，即风格是由一张具由显著风格的图像来进行控制的，本文称之为”风格参考图（reference style image）” + 本文只考虑图片生成，视频风格化不考虑</p><p>由于风格参考图以图像形式，不仅包含了风格信息，也包含了内容信息，因此经常会带来“内容泄露”：会把图片内容误认为是风格的一部分；针对此需要考虑如何将风格参考图中的“风格”与“内容”解耦。</p></blockquote><p><strong>刚好有篇style-transfer的纯论文梳理的工作</strong>：<a href="https://github.com/Westlake-AGI-Lab/Awesome-Style-Transfer-with-Diffusion-Models">Awesome-Style-Transfer-with-Diffusion-Models论文总结</a> ：包括了图像的风格迁移和视频的风格迁移，其中图像的风格迁移包括了文本驱动的和图像驱动的，细分又按照技术路线进行了分类。在此基础上，我根据自己的理解和行文逻辑进行了下文的梳理，会把论文的关键技术要素概括出来。</p><ol><li><p>按照生成内容的输入模态可以分为：</p><ul><li>文本驱动的</li><li>图像驱动的（较少、较难）</li></ul></li><li><p>按照技术路线分类：</p><ul><li>基于编码器encoder的：对风格特征进行提取并通过交叉注意力注入</li><li>基于反演Invertion的：较早起研究，效果一般，后文不介绍</li><li>基于注意力attention操作的：shared-attention、swap-attention</li></ul></li><li><p>按照每个风格的需求数据量分类：</p><ul><li>zero-shot</li><li>few-shot：风格LoRA TI  Dreamdooth——&gt; 风格LoRA还是最稳定的，但是如何减少风格集合带来的内容泄露也是一个可以优化的点</li></ul></li></ol><p>正文：</p><h2 id="基于编码器encoder："><a href="#基于编码器encoder：" class="headerlink" title="基于编码器encoder："></a>基于编码器encoder：</h2><ol><li><p><strong>IP-Adapter-2023.8:</strong></p><ul><li>分类：文本驱动的+图像驱动的、基于编码器encoder的、zero-shot</li><li>总结：交叉注意力的方式注入风格参考图片中的特征，但特征中的风格与内容无法进行解耦——内容泄露+无法稳定的风格化生成；且对每一张参考图片需要手动调节scale</li></ul></li><li><p><strong>DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations- CVPR 2024</strong></p><ul><li>分类：文本驱动的、基于编码器encoder的、zero-shot</li><li>总结：通过对Q-former的训练，实现用Q-former对风格参考图片的内容与风格特征解耦 + 交叉注意力方式注入风格特征；</li><li>缺点：<strong>风格是无穷多样的，经过有限的风格对训练数据训练的Q-former没办法真正的全面的学到风格特征提取；这也是一切用风格对pair训练数据进行风格解耦训练的方法的统一缺点</strong></li><li><img src="/../imgs/Research-Insights_Stylized-generation-based-diffusion/image-20250227154122749.png" alt="DEADiff-Q-former trained"></li></ul></li><li><p><strong>InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation-2024.4</strong></p><ul><li><p>分类：文本驱动的、基于编码器encoder的、zero-shot</p></li><li><p>总结：简单 又巧妙…</p><ul><li><strong>将风格参考图片的内容与风格解耦：CLIP-image-encoder对风格参考图片提取图片特征，CLIP-text-encoder对风格参考图片的内容描述提取内容特征，两个特征相减！【假设：认为在CLIP文本对齐的性质具由文图特征空间是同一个特征空间】</strong></li><li>对Unet分层研究：发现了特定层对风格的反应显著，命名为：layout block和style block；风格参考图的风格特征只注入这两个block【注入方式为IP-Adapter】</li></ul></li><li><p><strong>InstantStyle-Plus: Free Lunch towards Style-Preserving in Text-to-Image Generation-2024.6</strong> </p><ul><li>分类：<strong>图像驱动的</strong></li><li>用了各种辅助模块进行内容的保留，也就是内容参考图像&#x3D;&#x3D;&gt;图像驱动，包括ipadapter、tile_controlnet; 风格上额外引入风格鉴别器；</li></ul><p><img src="/../imgs/Research-Insights_Stylized-generation-based-diffusion/image-20250227200225157.png" alt="image-20250227200225157"></p></li><li><p><strong>CSGO:  CONTENT-STYLE COMPOSITION IN TEXT-TO IMAGE GENERATION</strong>  和instant style同一个团队</p><ul><li>分类：<strong>图像驱动的</strong>，基于编码器encoder的</li><li>总结：分别训练了针对内容的和针对风格的特征投影+交叉注意力；</li><li>缺点：利用了有限的风格内容pair对训练集，而风格是无穷的，是一切用风格对pair训练数据进行风格解耦训练的方法的统一缺点</li><li><img src="/../imgs/Research-Insights_Stylized-generation-based-diffusion/image-20250227205948912.png" alt="CSGO"></li></ul></li></ul></li></ol><h2 id="基于注意力attention操作的"><a href="#基于注意力attention操作的" class="headerlink" title="基于注意力attention操作的"></a>基于注意力attention操作的</h2><ol><li><p><strong>Style Aligned Image Generation via Shared Attention</strong>  -CVPR 2024  </p><ul><li><p>分类：文本驱动的、基于注意力attention的、zero-shot——&gt;<em>实际本文并不具有”风格参考图片“，但结合invertion可以将第一张生成的图片理解为风格参考图片</em>，除了原repo，<a href="https://github.com/AstitvaSri/Control-Style-Aligned-Generation.git">github上有人对controlnlet的invertion做出了实现</a></p></li><li><p>总结：**共享自注意力的思路很巧妙！！！midjourney的ref功能也许与这里相同？**可以利用生成过程中的注意力，迁移&#x2F;共享给后续的生成过程，以得到相似内容&#x2F;风格——多头注意力</p><ul><li><p>sharing attention layers across the generated images【self-attention】 </p></li><li><p>Adaptive Instance Normalization &#x3D; AdaIN :自适应实例归一化：风格迁移中常用的技术，通过调整内容图像的均值和标准差来匹配风格参考图像的均值和标准差，使得内容图像适应风格参考图像的风格分布【在传统风格迁移任务中，认为图片的均值、标准差在某程度上代表了抽象意义上的“风格”】</p></li><li><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">adain</span>(<span class="hljs-params">feat: T</span>) -&gt; T:<br>    feat_mean, feat_std = calc_mean_std(feat)<br>    <span class="hljs-comment"># 计算风格参考图的均值和方差</span><br>    feat_style_mean = expand_first(feat_mean) <span class="hljs-comment"># 抽取batch中索引为0的图片并扩展维度</span><br>    feat_style_std = expand_first(feat_std)<br>    <span class="hljs-comment"># 实例归一化</span><br>    feat = (feat - feat_mean) / feat_std<br>    feat = feat * feat_style_std + feat_style_mean<br>    <span class="hljs-keyword">return</span> feat<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">shared_call</span>(<span class="hljs-params">  self, attn: attention_processor.Attention,  </span><br><span class="hljs-params">  hidden_states, encoder_hidden_states=<span class="hljs-literal">None</span>,  attention_mask=<span class="hljs-literal">None</span>,   **kwargs</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.adain_queries:   <span class="hljs-comment"># AdaIN 操作</span><br>        query = adain(query)<br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.adain_keys:<br>        key = adain(key)<br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.adain_values:<br>        value = adain(value)<br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.share_attention:<br>    <span class="hljs-comment"># 与batch[0]==风格参考图片特征，进行拼接</span><br>        key = concat_first(key, -<span class="hljs-number">2</span>, scale=<span class="hljs-variable language_">self</span>.shared_score_scale) <span class="hljs-comment"># # [bs, heads, query_length * 2 , d_k]</span><br>        value = concat_first(value, -<span class="hljs-number">2</span>)                            <span class="hljs-comment"># # [bs, heads, query_length * 2 , d_k]</span><br>        <span class="hljs-comment"># 计算注意力</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.shared_score_shift != <span class="hljs-number">0</span>:<br>            hidden_states = <span class="hljs-variable language_">self</span>.shifted_scaled_dot_product_attention(attn, query, key, value,) <span class="hljs-comment"># [bs, heads, query_length , d_k]</span><br>        <span class="hljs-keyword">else</span>:<br>            hidden_states = nnf.scaled_dot_product_attention(<br>                query, key, value, attn_mask=attention_mask, dropout_p=<span class="hljs-number">0.0</span>, is_causal=<span class="hljs-literal">False</span><br>            )<br>    <span class="hljs-keyword">else</span>:<br>        hidden_states = nnf.scaled_dot_product_attention(<br>            query, key, value, attn_mask=attention_mask, dropout_p=<span class="hljs-number">0.0</span>, is_causal=<span class="hljs-literal">False</span><br>        )<br></code></pre></td></tr></table></figure></li></ul></li><li><p><img src="/../imgs/Research-Insights_Stylized-generation-based-diffusion/image-20250227162410941.png" alt="shared-attention with Adain"></p></li></ul></li><li><p><strong>Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer</strong>- CVPR 2023.12 Highlight</p><ul><li>分类：<strong>图像驱动的</strong>、基于注意力attention的、zero-shot、基于invertion的</li><li>总结：利用了invertion反演，也是在self-attention层进行操作：把风格的hidden_feature替换K&#x2F;V，把内容的hidden_feature与原始hidden_feature加权作为Q；<img src="/../imgs/Research-Insights_Stylized-generation-based-diffusion/image-20250227202947918.png" alt="image-20250227202947918"></li></ul></li></ol><p>总结：</p><ul><li><p>现阶段的很多工作还是文本驱动的；</p></li><li><p>图片驱动的对于内容、风格解耦与组合要求更复杂，目前实现要么效果不好、要么十分复杂；</p></li><li><p>few-shot的风格LoRA算是较为稳定的实现方式，但存在训练集的内容泄露问题；</p></li></ul><p><img src="/../imgs/Research-Insights_Stylized-generation-based-diffusion/image-20250227204852267.png" alt="多种风格化生成对比-文本驱动"></p><h1 id="补充：FLUX-1-Redux迁移—Redux-pro重风格化restyling"><a href="#补充：FLUX-1-Redux迁移—Redux-pro重风格化restyling" class="headerlink" title="补充：FLUX.1 Redux迁移—Redux_pro重风格化restyling"></a>补充：FLUX.1 Redux迁移—Redux_pro重风格化restyling</h1><p>flux.1模型以及[flux.1 tools模型介绍](<a href="https://blackforestlabs.ai/flux-1-tools/?ref=blog.comfy.org">Introducing FLUX.1 Tools - Black Forest Labs</a>)：</p><ul><li><strong>flux.1 Fill : Inpaint &#x2F; outpaint ：图像擦除、插入、 编辑与扩充</strong></li><li><strong>flux.1 Redux : An adapter that allows mixing and recreating input images and text prompts. 输入图像的Variation、restyling</strong></li><li>flux.1 Depth</li><li>flux.1 Canny</li></ul><p>综合应用：Redux+fill</p><ul><li>FLUX.1 Redux 提取图A主体的风格特征。</li><li>将图A与图B拼接，作为 FLUX.1 Fill 的重绘底图。</li><li>FLUX.1 Fill 在局部重绘时，将 FLUX.1 Redux 提供的图A特征条件与底图的图A特征对齐，最终生成结果高度还原图A的主体特征。</li></ul><p><img src="https://picx.zhimg.com/70/v2-00d0d772afd375663701a356926b5d2f_1440w.image?source=172ae18b&biz_tag=Post" alt="FLUX.1 超高还原风格迁移：将图像特征100%精准复刻到新场景的终极工作流"></p><p>综合应用：PuLID-FLUX-v0.9：换脸、人脸一致性</p>]]></content>
    
    
    <categories>
      
      <category>Research Insights</category>
      
    </categories>
    
    
    <tags>
      
      <tag>diffusion</tag>
      
      <tag>text-to-img</tag>
      
      <tag>style-transfer</tag>
      
      <tag>stylized-generation</tag>
      
      <tag>img-to-img</tag>
      
      <tag>大模型</tag>
      
      <tag>风格化生成</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper-Reading：MV_Adapter_multi-view_consistent_image_generation_made_easy- 2024.12</title>
    <link href="/paper-reading/Paper-Reading_MV-Adapter-multi-view-consistent-image-generation-made-easy/"/>
    <url>/paper-reading/Paper-Reading_MV-Adapter-multi-view-consistent-image-generation-made-easy/</url>
    
    <content type="html"><![CDATA[<h1 id="Paper-Reading：MULTI-VIEW-CONSISTENT-IMAGE-GENERATION-MADE-EASY-2024-12"><a href="#Paper-Reading：MULTI-VIEW-CONSISTENT-IMAGE-GENERATION-MADE-EASY-2024-12" class="headerlink" title="Paper-Reading：MULTI-VIEW CONSISTENT IMAGE GENERATION MADE EASY - 2024.12"></a>Paper-Reading：MULTI-VIEW CONSISTENT IMAGE GENERATION MADE EASY - 2024.12</h1><p><strong>关键概括：<strong>训练了一个即插即用的adapter，能够在各种diffusion模型下，通过相机参数或者几何信息对视角进行指导，进行</strong>多视角的一致性角色生成</strong>。</p><ul><li><strong>decoupled attention mechanism</strong>：新的自注意力层来源于对原自注意力层的复制 + 平行的自注意力连接架构【原spatial_self_attention与多个其他的attention平行连接！】</li><li><strong>condition-guider</strong>：能够对相机参数和几何信息进行编码的“新的条件编码器(condition encoder)”</li></ul><p><strong>优点：</strong></p><ul><li>即插即用，尽可能保留了底模(base model)的性能</li><li>训练容易，只需要微调几个adapter层而不是整个UNet：参数量小，数据集友好</li></ul><h2 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h2><h3 id="1-pipeline"><a href="#1-pipeline" class="headerlink" title="1. pipeline:"></a>1. pipeline:</h3><p>包括一个condition-guider encoder; 和decoupled attention mechanism</p><p><img src="/../imgs/Paper-Reading-MV-Adapter-multi-view-consistent-image-generation-made-easy/image-20250223194550810.png" alt="image-20250223194550810"></p><hr><h3 id="2-condition-guider-encoder："><a href="#2-condition-guider-encoder：" class="headerlink" title="2. condition-guider&#x3D;&#x3D;&gt; encoder："></a>2. condition-guider&#x3D;&#x3D;&gt; encoder：</h3><ul><li>conditioning map：<ul><li>相机位姿：raymap</li><li>几何信息：position_map + normal_map</li></ul></li><li>encoder结构：一系列的卷积网络&#x3D;&#x3D;&gt;包括feature extraction blocks特征提取 + downsampling layers下采样调整scale</li></ul><h3 id="3-decoupled-attention-mechanism："><a href="#3-decoupled-attention-mechanism：" class="headerlink" title="3. decoupled attention mechanism："></a>3. decoupled attention mechanism：</h3><ul><li><p>自注意力与其他注意力的连接方式不再是原始的级联，如下图所示：<img src="/../imgs/Paper-Reading-MV-Adapter-multi-view-consistent-image-generation-made-easy/image-20250223200818183.png" alt="serial attention-architecture"></p></li><li><p>并联架构，如下图所示：<img src="/../imgs/Paper-Reading-MV-Adapter-multi-view-consistent-image-generation-made-easy/image-20250223200121201.png" alt="平行注意力架构公式"></p></li><li><p>级联注意力架构  vs  并联注意力架构，对比如下图所示：</p></li></ul><p><img src="/../imgs/Paper-Reading-MV-Adapter-multi-view-consistent-image-generation-made-easy/image-20250223200148615.png" alt="级联注意力架构vs平行注意力架构"></p><ul><li>其中：<ul><li>spatial self-attention：即原始的自注意力层，Q&#x3D;K&#x3D;V&#x3D;hidden states，表示图片本身不同位置的关注度；</li><li><strong>multi-view attention</strong>：较为复杂，是多视图的实现关键！<img src="/../imgs%5CPaper-Reading-MV-Adapter-multi-view-consistent-image-generation-made-easy%5Cimage-20250223202102134.png" alt="image-20250223202102134"></li><li>image cross-attention：Q &#x3D; hidden states, K&#x3D;V&#x3D;来自冻结UNet对参考输入图提取得中间表示，即把冻结的平行UNet【被叫做UNet Encoder】作为了一种特征提取器，提取低级语义的特征和细粒度的细节；</li></ul></li></ul><h3 id="训练："><a href="#训练：" class="headerlink" title="训练："></a>训练：</h3><ul><li><p>数据集：Objaverse dataset</p></li><li><p>randomly zero out the features of the reference image to drop image conditions, enabling classifier-free guidance at inference：解释：对于CFG的训练策略，是在训练过程中按照比例（如0.1）随机丢弃<strong>条件信息</strong>，这里的“条件信息”一般是额外的(<strong>cross attn的K、V来源？</strong>)指导信息，比如文生图中的文本(text embedding)，这里丢弃的是<strong>参考图像特征</strong></p></li></ul><h3 id="待解决："><a href="#待解决：" class="headerlink" title="待解决："></a>待解决：</h3><ul><li><p>shift the noise schedule towards high noise levels</p></li><li><p>shift the log signal-to-noise ratio by log(n), where n is the number of generated views.</p></li><li><p><strong>multi-view attention</strong>部分没有细致的研究，row-wise self_attention column-wise self-attention是什么？</p></li></ul><h3 id="测试实例："><a href="#测试实例：" class="headerlink" title="测试实例："></a>测试实例：</h3><p>输入、输出&#x3D;768分辨率</p><p><img src="/../imgs/Paper-Reading-MV-Adapter-multi-view-consistent-image-generation-made-easy/image.webp" alt="预处理后的输入图"></p><p><img src="/../imgs/Paper-Reading-MV-Adapter-multi-view-consistent-image-generation-made-easy/image-1740315153443-3.webp" alt="6视图-1"></p><p>![img](..&#x2F;imgs&#x2F;Paper-Reading-MV-Adapter-multi-view-consistent-image-generation-made-easy&#x2F;image (5).webp)</p><p>![image (3)](..&#x2F;imgs&#x2F;Paper-Reading-MV-Adapter-multi-view-consistent-image-generation-made-easy&#x2F;image (3).webp)</p><p>![image (4)](..&#x2F;imgs&#x2F;Paper-Reading-MV-Adapter-multi-view-consistent-image-generation-made-easy&#x2F;image (4).webp)</p><p>![image (1)](..&#x2F;imgs&#x2F;Paper-Reading-MV-Adapter-multi-view-consistent-image-generation-made-easy&#x2F;image (1).webp)</p>]]></content>
    
    
    <categories>
      
      <category>paper_reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>diffusion</tag>
      
      <tag>text-to-img</tag>
      
      <tag>论文阅读</tag>
      
      <tag>multi-view</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>算法——leetcode</title>
    <link href="/LeetCode-highlights/LeetCode-Highlights/"/>
    <url>/LeetCode-highlights/LeetCode-Highlights/</url>
    
    <content type="html"><![CDATA[<p>刷题以及快速刷题的技巧：</p><p>先掌握好数据结构以及对应的常考算法，对应着下述刷题规划网站思考+看题解；先在心里大概记忆算法和对应的示例题目，快速了解算法。</p><p>自己计算空间复杂度、时间复杂度；</p><h2 id="算法讲解、刷题路线规划：好用网站"><a href="#算法讲解、刷题路线规划：好用网站" class="headerlink" title="算法讲解、刷题路线规划：好用网站"></a>算法讲解、刷题路线规划：好用网站</h2><ol><li><a href="https://programmercarl.com/">代码随想录</a></li><li><a href="https://labuladong.online/algo/home/">labuladong算法笔记</a></li><li><a href="https://github.com/CyC2018/CS-Notes.git">github-技术面试及刷题整理-CS-Notes</a></li></ol><p>总体路线：数据结构 + 算法：经典题</p><h2 id="算法书籍："><a href="#算法书籍：" class="headerlink" title="算法书籍："></a>算法书籍：</h2><ol><li>剑指offer 系列</li></ol><h2 id="算法知识点-简记"><a href="#算法知识点-简记" class="headerlink" title="算法知识点-简记"></a>算法知识点-简记</h2><h3 id="·-数据结构"><a href="#·-数据结构" class="headerlink" title="· 数据结构"></a>· 数据结构</h3><ol><li><p>数组：</p><ol><li>特点：内存连续，查询快速，增删复杂；</li><li>算法要点：二分查找，<strong>双指针</strong>（覆盖移除、快慢双指针、左右双指针），<strong>前缀和</strong>&#x2F;区间和；.sort()原地排序；</li><li>二分法：有序数组的mid中位数为界定的：<em>排除法</em>！！时间复杂度O(logn)</li></ol></li><li><p>链表：单链表、双向链表、循环链表；</p><ol><li><p>特点：内存不联系，查询慢速，增删容易；</p></li><li><p>算法要点：虚拟头结点、构建类（增删查改）、双指针（快n步双指针）、环形链表（数学推理）；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ListNode</span>:     <span class="hljs-comment"># 单链表</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, val=<span class="hljs-number">0</span>, <span class="hljs-built_in">next</span>=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-variable language_">self</span>.val = val<br>        <span class="hljs-variable language_">self</span>.<span class="hljs-built_in">next</span> = <span class="hljs-built_in">next</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ListNode</span>:     <span class="hljs-comment"># 双链表</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, val=<span class="hljs-number">0</span>, prev=<span class="hljs-literal">None</span>, <span class="hljs-built_in">next</span>=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-variable language_">self</span>.val = val<br>        <span class="hljs-variable language_">self</span>.prev = prev<br>        <span class="hljs-variable language_">self</span>.<span class="hljs-built_in">next</span> = <span class="hljs-built_in">next</span><br></code></pre></td></tr></table></figure></li></ol></li><li><p>哈希表&#x2F;散列表：<strong>数组、set集合、map映射</strong></p><ol><li><p>特点：查询特别快O(1)、底层包括哈希映射+哈希碰撞；</p><blockquote><p>当需要查询一个元素是否出现过、或者一个元素是否在集合里时，第一时间想到哈希法</p></blockquote></li><li><p>hash table的3中结构区别：</p><ul><li>作为哈希表的数组：已知数据长度<strong>才能且最好用</strong>数组，更快（不需要哈希映射计算）；</li><li>作为哈希表的set：集合，只存储key，需要哈希映射运算去计算内存地址；</li><li>作为哈希表的map【dict字典】：【key-value】结构；</li></ul></li><li><p>算法要点：先构建+再快速查询、n数和、结合双指针；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict, Counter<br><br><span class="hljs-built_in">dict</span>.get(get_val, set_val_if_cannot_get)<br><br><span class="hljs-built_in">set</span>() &amp; <span class="hljs-built_in">set</span>() <span class="hljs-comment"># 集合相与</span><br></code></pre></td></tr></table></figure></li></ol></li><li><p>字符串：</p><ol><li><p>算法要点：切片[::-1]、<strong>KMP前缀匹配-next前缀表</strong>O(m+n)、双指针；</p><blockquote><p>前缀表&#x3D;最长公共前后缀长度表&#x3D;next数组【原始、右移、减一】：j&#x3D;next[index-1] </p><p>讲解：<a href="https://programmercarl.com/0028.%E5%AE%9E%E7%8E%B0strStr.html">实现strStr() KMP前缀表+next数组、prefix数组</a></p></blockquote></li></ol></li><li><p>栈与队列：</p><ol><li>特点：<ul><li><strong>栈—实现了—&gt;递归</strong>：先进后出；出栈.pop()、查询栈顶[-1]</li><li>队列：先进先出；入队.append()、查询队首[0] <code>from collections import deque;   .popleft()</code></li><li>优先级队列&#x3D;堆：大顶堆、小顶堆（<strong>完全二叉树+且树中每个结点的值都不小于（或不大于）其左右孩子的值</strong>）<code>import heapq;   heapq.heappop(pri_que)</code></li></ul></li><li>算法要点：<strong>单调队列:push之前把前面小的删掉，队列永远是单调递减的</strong>：.pop()、单调栈【下一更大元素】时间复杂度O(n)</li></ol></li><li><p>二叉树：</p><ol><li><p>种类：</p><ol><li>满二叉树：如果一棵二叉树只有度为0的结点和度为2的结点，并且度为0的结点在同一层上，则这棵二叉树为满二叉树</li><li>完全二叉树：在完全二叉树中，除了最底层节点可能没填满外，其余每层节点数都达到最大值，并且最下面一层的节点都集中在该层最左边的若干位置。若最底层为第 h 层（h从1开始），则该层包含 1~ 2^(h-1) 个节点<ul><li>所有层（除了最后一层）都完全填满。</li><li>最后一层的节点尽可能靠左填充。</li><li>不要求所有叶子节点在同一层。</li></ul></li><li>完美二叉树、完整二叉树:<ul><li>所有层都完全填满。</li><li>所有叶子节点都在同一层。</li><li>每个非叶子节点都有两个子节点。</li></ul></li><li>二叉搜索树：<ol><li>若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值；</li><li>若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值；</li><li>它的左、右子树也分别为二叉排序树</li></ol></li><li>平衡二叉树：二叉树每个节点的左右两个子树的**高度【后序遍历】**差的绝对值不超过1;</li></ol></li><li><p>存储：</p><ol><li><p>链式存储：左右指针 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TreeNode</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, val, left = <span class="hljs-literal">None</span>, right = <span class="hljs-literal">None</span></span>):<br>        <span class="hljs-variable language_">self</span>.val = val<br>        <span class="hljs-variable language_">self</span>.left = left<br>        <span class="hljs-variable language_">self</span>.right = right<br></code></pre></td></tr></table></figure></li><li><p>顺序存储：内存是连续分布——数组</p></li></ol></li><li><p>遍历：</p><ol><li>深度优先遍历：<ul><li>前序遍历（递归法，迭代法<strong>用栈去实现</strong>）</li><li>中序遍历（递归法，迭代法<strong>用栈去实现</strong>）</li><li>后序遍历（递归法，迭代法<strong>用栈去实现</strong>）</li><li><em>二叉树的递归3步：确定递归函数的参数和返回值–&gt;确定终止条件–&gt;确定单层递归的逻辑</em></li></ul></li><li>广度优先遍历：<ul><li>层次遍历（迭代法<strong>用队列取实现</strong>）<code>queue = collections.deque([root]); queue.append(); queue.popleft()</code></li></ul></li></ol></li><li><p>深度、高度：求深度可以从上到下去查&#x3D;&#x3D;&gt;前序遍历（中左右），高度只能从下到上去查&#x3D;&#x3D;&gt;后序遍历（左右中）</p><p><img src="https://code-thinking-1253855093.file.myqcloud.com/pics/20210203155515650.png" alt="110.平衡二叉树2"></p></li><li><p>算法要点：遍历（前中后**、层序遍历**）、深度&#x2F;高度、节点个数、<strong>路径【回溯】</strong>、构造与修改、公共祖先; 递归 VS 迭代 <a href="https://programmercarl.com/0257.%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%89%80%E6%9C%89%E8%B7%AF%E5%BE%84.html#%E5%85%B6%E4%BB%96%E8%AF%AD%E8%A8%80%E7%89%88%E6%9C%AC">二叉树的所有路径：回溯</a>、左叶子之和，平衡二叉搜索树BST的构建；</p></li></ol></li></ol><h3 id="·-算法思想"><a href="#·-算法思想" class="headerlink" title="· 算法思想"></a>· 算法思想</h3><ol><li><p>排序</p></li><li><p>回溯</p><ol><li><p>适用问题：<strong>组合问题和分割问题都是收集树的叶子节点，而子集问题是找树的所有节点！</strong></p><ul><li>组合问题：N个数里面按一定规则找出k个数的集合：</li><li>切割问题：一个字符串按一定规则有几种切割方式</li><li>子集问题：一个N个数的集合里有多少符合条件的子集</li><li>排列问题：N个数按一定规则全排列，有几种排列方式：每层都从0开始搜索而不是startIndex、需要used数组记录已使用过元素集；</li><li>棋盘问题：N皇后，解数独等等</li></ul></li><li><p>算法特点：递归（本质是穷举）<strong>回溯可以被抽象为树形结构【N叉树】</strong>；场景：在同一个&#x2F;不同多个 集合中递归查找子集，<strong>集合的大小就构成了树的宽度，递归的深度就构成了树的深度</strong>。 —— “N叉树的层序遍历：<strong>for循环横向遍历，递归纵向遍历，回溯不断调整结果集</strong>。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs text">3步曲：<br>1. 回溯函数的模板返回值【void】以及参数<br>2. 回溯函数的终止条件<br>3. 回溯搜索的遍历过程<br><br>void backtracking(参数) &#123;<br>    if (终止条件) &#123;<br>        存放结果;<br>        return;&#125;<br>    for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) &#123;<br>        处理节点;<br>        backtracking(路径，选择列表); // 递归<br>        回溯，撤销处理结果<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p><img src="https://code-thinking-1253855093.file.myqcloud.com/pics/20210130173631174.png" alt="回溯算法理论基础"></p></li><li><p>注意：</p><ul><li>集合中的元素不能重复使用时：需要startIndex调整下一层递归的起始位置；</li><li><strong>横向去重【排序后+used标记、for外used哈希表&#x2F;哈希数组标记】</strong>、纵向去重[startIndex]；</li></ul></li></ol></li><li><p>贪心</p><ol><li>特点：<strong>选择每一阶段&#x2F;状态的局部最优，从而达到全局最优</strong></li><li>经典例题：跳跃游戏、重叠区间（先排序）、发饼干</li></ol></li><li><p>动态规划</p><ol><li><p>特点：<strong>需要对前一个状态进行推导</strong></p></li><li><p>动态规划5步曲：</p><ul><li><p>确定dp数组（dp table）以及下标的含义</p></li><li><p>确定递推公式</p></li><li><p>dp数组如何初始化</p></li><li><p>确定遍历顺序</p></li><li><p>举例推导dp数组——打印dp数组debug</p><pre><code class="hljs">      # 动态规划：      # 1. 结合题干-dp含义:dp      # 2. 递归公式       # 3. 初始化       # 4. 遍历顺序      # 5. 打印dp数组</code></pre></li></ul><p><code> </code></p></li><li><p>经典例题：斐波那契、爬楼梯、整数拆分<img src="https://kstar-1253855093.cos.ap-nanjing.myqcloud.com/baguwenpdf/_%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE_%E9%9D%92.png" alt="img"></p></li><li><p>打家劫舍问题——【树形dp遍历】</p></li><li><p>股票问题——</p></li><li><p>子序列、子数组&#x3D;&#x3D;连续子序列 问题：</p><pre><code class="hljs">    # 子序列  vs  子数组：子数组要求连续！！！    最长公共子序列：二维dp    # 子序列：dp[i][j]表示nums1:0~i-1 与 nums2:0~j-1的最长公共子的长度    # 递归公式：if nums1[i-1]==nums2[j-1]:dp[i][j] = dp[i-1][j-1]+1, else: max(dp[i-1][j], dp[i][j-1])        # 子数组==连续子序列：dp[i][j]表示以nums1[i-1] 与 nums2[j-1]结尾的、最长公共子数组的长度    # 递归公式：if nums1[i-1]==nums2[j-1]:dp[i][j] = dp[i-1][j-1]+1, else:0</code></pre></li><li><p>编辑距离问题——增删改: 二维dp数组；</p></li><li><p>回文子串问题——布尔类型的dp[i]-[j]：表示区间范围[i,j] （注意是左闭右闭）的子串是否是回文子串，如果是：dp[i]-[j]为true，否则为false</p></li><li><p>背包问题——二维dp数组  or 一维dp数组：<strong>dp[i]-[j] 表示从下标为[0-i]的物品里任意取，放进容量为j的背包，价值总和最大是多少：dp[i]·[j] &#x3D; max( dp[ i - 1 ]·[ j ] ,     dp[ i - 1 ]·[ j- weight[ i ] ] + value[ i ] )</strong></p><ul><li>01背包：一维dp数组：先物品 再倒序背包</li><li>完全背包：一维dp数组：组合{先物品 再正序背包}、排列{先正序背包 再物品}</li></ul><p><img src="https://code-thinking-1253855093.file.myqcloud.com/pics/20210117171307407.png" alt="416.背包问题-分割等和子集1"></p></li></ol></li><li><p>图论</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>LeetCode highlights</category>
      
    </categories>
    
    
    <tags>
      
      <tag>leetcode刷题</tag>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Research Insights: diffusion VS control</title>
    <link href="/Research-Insights/Research-Insights_Personalization-and-feature-Mechanism/"/>
    <url>/Research-Insights/Research-Insights_Personalization-and-feature-Mechanism/</url>
    
    <content type="html"><![CDATA[<h1 id="Research-Insights-diffusion-VS-control"><a href="#Research-Insights-diffusion-VS-control" class="headerlink" title="Research Insights: diffusion VS control"></a>Research Insights: diffusion VS control</h1><p>文章目的：从大四接触diffusion到现在已经接近2年，温故而知新，最近在回顾之前阅读过的工作，也总结一下在control方面的技术发展路线，看看能否给带来点启发。谈到control技术的分类，很多工作被统一归纳为“encoder-based methods”，但这种归纳还是太”大“了，这里用我自己的理解做一些更细致的分类，分类依据更偏向于具体的<em>特征注入机制&#x2F;控制机制</em>。</p><hr><p>encoder-based methods如何理解？：从特征的提取encoder角度入手，着重于将“<strong>不同的image-feature以及不同的特征提取方式</strong>” + “<strong>不同的control控制机制</strong>” 做排列组合。</p><p>比如 IC-light 使用可训练的 MLP 提取环境贴图的hdr-envmap-embedding + <strong>stacked into text_embedding</strong>控制机制，</p><p>比如 Instant-ID 使用人脸识别器提取face-embedding + <strong>IP-A</strong> 、人脸关键点facial-keypoints + <strong>ControlNet</strong> 控制机制，</p><p>比如 Anydoor 使用：sobel算子提取的高频信息map与背景和位置+<strong>ControlNet</strong>控制机制、DINO-V2提取的特征+<strong>stacked into text_embedding</strong>控制机制，</p><p>比如 AnyText 使用OCR文字识别器提取glyph-embedding + <strong>ControlNet</strong>控制机制……</p><p>似乎是只要找到能用来提取特征的特征提取器（一般借鉴传统算法），再结合某种特定的控制机制，就能注入各种形式的条件特征。本文不介绍“不同的image-feature以及不同的特征提取方式”，着重介绍 “不同的control控制机制” 的技术路线。</p><hr><p>条件控制的文生图目标：期待在文本提示基础上，能够参考到来自图像的提示，毕竟有些提示不是言辞能够表达的，“词不达意”，图片能够蕴含更多信息也更贴近人类视觉观察的维度。本文的control主要是指除了文本提示以外的其他提示（即，图像）。</p><blockquote><p>回顾文本条件注入：基础的文生图模型，如LDM(latent diffusion model)中文本条件注入机制：text-encoder + cross-attention[text-embedding作为Key&#x2F;Value]；</p><p>回顾微调：最初的探索阶段涌现出的微调手段，如Dreambooth，Textual-Inversion，LoRA…目前LoRA依然具有很强的应用能力。</p></blockquote><h2 id="1-ControlNet系列"><a href="#1-ControlNet系列" class="headerlink" title="1. ControlNet系列"></a>1. ControlNet系列</h2><p>太经典了，不必多说：ControlNet引入结构控制：zero-initialization + copyed half_unet,  residual思想：直接相加；💥</p><p>论文举例：（略）</p><ul><li>ControlNet</li><li>T2I-adapter</li><li>Uni-ControlNet</li></ul><blockquote><p>注：基本没使用 image-encoder，不算是encoder-based的一种，但经常与encoder-based结合，可以成为controlnet-based？毕竟controlnet就是非常简单粗暴，直接copy后就开始无脑学…</p></blockquote><h2 id="2-stacked-into-text-token-基本不再使用"><a href="#2-stacked-into-text-token-基本不再使用" class="headerlink" title="2. stacked into text_token-基本不再使用"></a>2. stacked into text_token-基本不再使用</h2><p>概括：把<strong>CLIP image encoder</strong>提取到的图像特征作为文本，替换text中的伪词，然后得到新的融合text_embedding，作为 cross-attention的 Key&#x2F;Value 注入unet，以指导图像生成。由于需要额外训练text-encoder，此策略基本已被抛弃，虽然但是，下面两篇文章与IP-A时间相近，都利用了解耦的cross-attention！</p><ul><li><p><a href="http://arxiv.org/abs/2302.13848">ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation</a>-2023.8</p><ul><li>策略：发现本文发行的时间与IP-A很近？也算是IP-A类似，不过在text-cross-attention中掺杂了<strong>stacked into text_embedding</strong>的类似策略，可以说是stacked into text_token；现在一般都是在embedding层面的融入，即在text-encoder之后的融入。</li></ul></li><li><p><a href="http://arxiv.org/abs/2309.05793">PhotoVerse: Tuning-Free Image Customization with Text-to-Image Diffusion Models</a>-2023.9</p></li></ul><p><img src="/../imgs/personalization-and-feature-mechanism/image-20250109172222535.png" alt="左图为PhotoVerse-右图为ELITE" title="左图为PhotoVerse-右图为ELITE"></p><h2 id="3-stacked-into-text-embedding"><a href="#3-stacked-into-text-embedding" class="headerlink" title="3. stacked into text_embedding:"></a>3. stacked into text_embedding:</h2><p>概括：把<strong>CLIP image encoder</strong>提取到的图像特征，与文本特征向量（即text_embedding）<strong>拼接concatenate</strong>或者<strong>替换replace</strong>，得到的融合特征作为 cross-attention的 Key&#x2F;Value 注入unet，以指导图像生成。</p><p>缺点：生成的图像只是部分忠实于图像提示，对图像提示的表现力不如微调（如LoRA）。</p><blockquote><p>与上段 stacked into text_token 的区别是：clip-image-feature是在哪个维度与text条件融合的，一个是在token甚至是纯文本阶段，一个是在文本嵌入向量阶段。前者基本已被淘汰，后者在特征空间的维度上融合更合理。</p></blockquote><p>论文举例：</p><ul><li><p><a href="http://arxiv.org/abs/2307.09481">AnyDoor: Zero-shot Object-level Image Customization</a>-2023.7 阿里巴巴 蚂蚁</p><ul><li><p>策略：sobel算子提取的高频信息map与背景和位置+<strong>ControlNet</strong> 、DINO-V2提取物体的信息特征+<strong>stacked into text_embedding</strong>；</p><p><img src="/../imgs/personalization-and-feature-mechanism/image-20250109154914240.png" alt="image-20250109154914240" title="anydoor结构"></p></li></ul></li><li><p><a href="http://arxiv.org/abs/2312.04461">PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding</a>-2023.12  已开源</p><ul><li><p>策略：利用CLIP-image-encoder提取具有人像信息的image-feature，经过<strong>MLP</strong>，替换原始文本中“男人”或者“女人”对应的embedding作为新的“text_embedding”。</p></li><li><p>优点：由于保持了text_embedding的存在，对原始底膜的语义遵循没太大影响，语义一致性不错；</p></li><li><p>缺点：仅使用更新的带有人像信息的“text_embedding”，对人像信息控制不充分，人像保持差；</p><p><img src="/../imgs/personalization-and-feature-mechanism/image-20250109113515713.png" alt="image-20250109113515713"></p></li></ul></li></ul><h2 id="4-cross-attention-mechanism✨"><a href="#4-cross-attention-mechanism✨" class="headerlink" title="4. cross-attention mechanism✨:"></a>4. cross-attention mechanism✨:</h2><p>概括：在上文中提到的注意力机制中，每个注意力层都只包括了1个self-attention、1个cross-attention，并在这1个cross-attention中注入”由文本提升和图像提示融合得到的条件特征”【融合条件特征作为cross-attention的 Key&#x2F;Value】。由此出发的改进策略：将<strong>注意力解耦</strong>，即把“文本条件特征”与“图像条件特征”分开，分别注入到2个不同的cross-attention中去，再进行相加。</p><p>优点：图像提示拥有了与文本提示“同等地位”的控制权，能够更好的听图像提示的话！</p><p>论文举例：</p><ul><li><p><a href="http://arxiv.org/abs/2308.06721">IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models</a>-2023.8 腾讯✨</p><ul><li><p>策略：利用<strong>CLIP-image-encoder</strong>提取具有图像的image-feature，经过<strong>Linear+LayerNorm</strong>，将<strong>注意力解耦</strong>，即把“文本条件特征”与“图像条件特征”分开，分别注入到2个不同的cross-attention中去[ <em>text-cross-attention与image-cross-attention</em> ]，再将2个cross-attention结果进行相加。</p><p><img src="/../imgs/personalization-and-feature-mechanism/image-20250109143750831.png" alt="image-20250109143750831"></p></li></ul></li><li><p><a href="http://arxiv.org/abs/2403.11781">Infinite-ID: Identity-preserved Personalization via ID-semantics Decoupling Paradigm</a>-2024.3 中科大 没开源</p><ul><li>策略：与IP-A类似，只不过在imgae-feature提取这有所增添：除了来自CLIP-image-encoder提取的人像信息【后续称为clip-image-embedding】，还使用了来自<strong>人脸识别器</strong>提取的face-embedding，并将两者<strong>拼接concat</strong>，拼接后一齐作为图像条件特征（<strong>IP-A</strong>）。</li><li>训练策略：训练时，只使用图像输入，不使用caption。据说能够增强对图像的学习。</li><li>除此之外，风格控制方面还使用了AdaIN-m机制，主要是在self-attention上做了改动。</li><li><img src="/../imgs/personalization-and-feature-mechanism/image-20250109152153105.png" alt="image-20250109152153105"></li></ul></li><li><p><a href="http://arxiv.org/abs/2401.07519">InstantID: Zero-shot Identity-Preserving Generation in Seconds</a>-2024.2 InstantX、小红书</p><ul><li><p>策略：与Infinite-ID类似，也是在图像特征提取上做了改动：直接放弃CLIP-image-encoder，只使用人脸模型提取的face-embedding作为图像条件特征（<strong>IP-A</strong>）。除此之外，还使用了人脸关键点图结合<strong>ControlNet</strong>进行人脸五官位置的结构控制。</p></li><li><blockquote><p>注意：ControlNet不再使用text-embedding，只使用face-embedding。</p></blockquote></li></ul></li></ul><h2 id="5-double-UNet-mechanism✨"><a href="#5-double-UNet-mechanism✨" class="headerlink" title="5. double UNet mechanism✨:"></a>5. double UNet mechanism✨:</h2><p>概括：经过大量数据预训练的diffusion model本身已经具有了很强大的对图像提取特征的能力，那可以直接拿来替换&#x2F;作为上文的各种image-encoder啊！具体从哪层”拿“还挺值得研究，下面的两篇都是拿的ref_unet中的self-attention输出的feature，<strong>？？？？为什么呢</strong>。</p><p>论文举例：</p><ul><li><p><a href="http://arxiv.org/abs/2306.00973">Intelligent Grimm – Open-ended Visual Storytelling via Latent Diffusion Models</a>-2024.3 已开源✨</p><ul><li>策略：可以理解为IP-A的变种：依然是解耦的cross-attention，不过新增的image-cross-attention的keys&#x2F;values不再是直接用image-encoder提取的image-feature，用的是参考图像的”ref_unet”流程中<strong>ref-self-attention后得到的ref-unet-feature</strong>。</li></ul><p><img src="/../imgs/personalization-and-feature-mechanism/image-20250109164719609.png" alt="image-20250109164719609"></p></li><li><p>[Improving Diffusion Models for Authentic Virtual Try-on in the Wild]([<a href="https://arxiv.org/abs/2403.05139">2403.05139] Improving Diffusion Models for Authentic Virtual Try-on in the Wild</a>)-2024.3 KAIST 已开源✨</p><ul><li>策略：本文拿的也是”ref_unet”流程中<strong>ref-self-attention后得到的ref-unet-feature</strong>，与Intelligent Grimm不同的是：本文不再将其作为cross-attention的keys&#x2F;values，而是与hidden-states拼接作为新的Query作用到self-attention。</li><li>疑问：关于high-level、low-level如何理解？</li></ul></li></ul><p><img src="/../imgs/personalization-and-feature-mechanism/image-20250109173953044.png" alt="image-20250109173953044" title="IDM–VTON结构"></p><h2 id="6-novel-loss"><a href="#6-novel-loss" class="headerlink" title="6. novel loss"></a>6. novel loss</h2><p>在diffusion中常见的loss是：由unet预测到的噪声与真实噪声之间的MSE均方差loss，被称作vanilla-loss或naive-loss：</p><p><img src="/../imgs/personalization-and-feature-mechanism/image-20250109160403306.png" alt="image-20250109160403306"></p><p>论文举例：</p><ul><li><a href="http://arxiv.org/abs/2404.16022">PuLID: Pure and Lightning ID Customization via Contrastive Alignment</a>-2024.4 字节<ul><li>策略：利用加速模型（如Lightning T2I），在训练过程中增加对比loss，不对原模型能力做出破坏。<strong>IP-A</strong>+ <strong>Lightning T2I</strong>+使用额外的loss设计，包括在unet内部的aligh_loss和unet外部的id_loss；</li><li>优点：一个路径仅受提示的条件，而另一个路径使用ID和提示作为条件。通过在语义上对齐这两条路径上的UNET特征（即，Qt与Qtid），模型将学习如何在不影响原始模型行为的情况下嵌入ID。</li><li>缺点：增加耗时。？</li></ul></li></ul><p>​<img src="/../imgs/personalization-and-feature-mechanism/image-20250109161049551.png" alt="image-20250109161049551"></p><p>​<img src="/../imgs/personalization-and-feature-mechanism/image-20250109161355622.png" alt="image-20250109161355622"></p><p>待看：</p><p><a href="http://arxiv.org/abs/2406.16537">Character-Adapter: Prompt-Guided Region Control for High-Fidelity Character Customization</a>-2024.6</p>]]></content>
    
    
    <categories>
      
      <category>Research Insights</category>
      
    </categories>
    
    
    <tags>
      
      <tag>diffusion;</tag>
      
      <tag>control;</tag>
      
      <tag>text-to-img;</tag>
      
      <tag>img-to-img;</tag>
      
      <tag>大模型；</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>paper reading: Nested Attention: Semantic-aware Attention Values for Concept Personalization-2025</title>
    <link href="/paper-reading/Paper-Reading_Nested-Attention/"/>
    <url>/paper-reading/Paper-Reading_Nested-Attention/</url>
    
    <content type="html"><![CDATA[<h1 id="Nested-Attention-Semantic-aware-Attention-Values-for-Concept-Personalization-2025"><a href="#Nested-Attention-Semantic-aware-Attention-Values-for-Concept-Personalization-2025" class="headerlink" title="Nested Attention: Semantic-aware Attention Values for Concept Personalization-2025"></a><a href="https://snap-research.github.io/NestedAttention/"><strong>Nested Attention:</strong> Semantic-aware Attention Values for Concept Personalization-2025</a></h1><p>关键概括：injects a rich and expressive image representation into the model’s existing cross-attention layers. &#x3D;&#x3D;&gt;  <strong>single textual-token</strong>、<strong>cross-attention: Nested Attention</strong>、 <strong>smaller semantic visual elements</strong>、 <strong>多个定制化概念</strong>、<strong>可以非人 数据集低需求</strong>  </p><p>personalization技术发展路线：text-embedding&#x2F;fine-tuning based——&gt;cross-image attention based——&gt;<em><strong>encoder-based（本文）</strong></em>✨</p><span id="more"></span><h2 id="1-如何理解per-query-attention-values"><a href="#1-如何理解per-query-attention-values" class="headerlink" title="1. 如何理解per-query attention values?"></a>1. 如何理解per-query attention values?</h2><h3 id="回顾cross-attention"><a href="#回顾cross-attention" class="headerlink" title="回顾cross-attention"></a>回顾cross-attention</h3><p>Query: hidden_states(来自unet中间层)</p><p>Key &amp; Value: text_embedding(来自text-encoder)</p><p>Q 与 K 的点积意义：表示当前空间位置下的 Q_ij 与 K 的语义相似性，即权重，用来后续与 V 加权。由此，在给定文本（K &#x2F; V ）下，Q的能够决定图像内容的“空间位置”，也就是控制了图像的“外观”！这是文章着重研究 Query 的原因。</p><p><img src="/../imgs/Paper-Reading_Nested-Attention/image-20250108160404745.png" alt="cross-attention" title="cross-attention"></p><h3 id="per-query-attention-Values"><a href="#per-query-attention-Values" class="headerlink" title="per-query attention Values"></a>per-query attention Values</h3><p>原文提取：per-query attention Values &#x3D; localized values that depend on the queries&#x3D;per-region values&#x3D;query-dependent values</p><p>Value: text_embedding(来自text-encoder)，由于 Value由不同的token对应的embedding组成，而一个token却要指示着图片<em>整个区域</em>的全部相关实例和相关内容，这很“粗粒度”，无法达到任务期待的“细粒度”个性化生成【value中的每一个embedding要负责整个query，任务重，容易完成的不好】。因此把任务细分：划分query，每个子query由专门的新value负责，减轻了value的任务量。即：提出了更局部的“localized Values”：能更好的关注到局部区域、细粒度的语义信息。</p><p>所谓的“per-query attention Values”具体是怎样实现的就是下面的内容了。</p><blockquote><p>注意：per-query attention Values ≠ attention map的值，Values指的是Q K V中的V。</p></blockquote><h2 id="2-如何理解nested-attention-mechanism？"><a href="#2-如何理解nested-attention-mechanism？" class="headerlink" title="2. 如何理解nested attention mechanism？"></a>2. 如何理解nested attention mechanism？</h2><p>其中公式1就是上文的“per-query attention Values”的实现方式了。简单来说，就是对special token（s*）在不同的空间位置（i,j）下的q_ij，单独预测value_ij，这样得到的value_ij便具有了更局部的、细粒度的语义信息。但不是所有的text token都是用这个机制，只有要被个性化的special token会用到，这种注意力机制就是nested attention mechanism。</p><p><img src="/../imgs/Paper-Reading_Nested-Attention/image-20250108165121686.png" alt="公式1&#x2F;2&#x2F;3" title="公式1&#x2F;2&#x2F;3"></p><blockquote><p>注意，Key 在公式1和3中的区别！(从左到右分别：公式1&#x2F;2&#x2F;3)<br>文中还提到了对“per-query attention Values”的正则化实验技巧，不具体介绍。</p></blockquote><h2 id="3-q-ij、nested-keys、nested-values从哪来？——可训练模块"><a href="#3-q-ij、nested-keys、nested-values从哪来？——可训练模块" class="headerlink" title="3. q_ij、nested keys、nested values从哪来？——可训练模块"></a>3. q_ij、nested keys、nested values从哪来？——可训练模块</h2><p>Q-Former得到：”Q-Former learned queries“，即q_ij；</p><p>nested attention layers[ linear layers ]得到：nested keys、nested values；</p><p>上述两个模块组成了文章的可训练部分，得到的q_ij、nested keys、nested values三者构成公式1的输入。</p><blockquote><p>注意：per-query attention Values ≠ nested values, 二者关系：nested values 和 nested keys 经过公式1 得到per-query attention Values。</p><p>clip image features &#x3D; CLIP ‘s last layer before pooling</p></blockquote><p><img src="/../imgs/Paper-Reading_Nested-Attention/image-20250108172150828.png" alt="论文架构"></p><h2 id="4-对“Q-Former-learned-queries”的验证："><a href="#4-对“Q-Former-learned-queries”的验证：" class="headerlink" title="4. 对“Q-Former learned queries”的验证："></a>4. 对“Q-Former learned queries”的验证：</h2><p>从生成过程中的Query中取3个不同空间位置的q_ij，与nested keys进行点积运算得到attention map’，可以观察到总能有1-2个nested token与q_ij最相关；进一步将q_ij、nested keys、nested values按照公式1进行运算，得到Q-Former learned queries，与输入脸部图像的clip image features 进行点积运算得到attention map, 能直观的观察到Q-Former learned queries的作用，即生成的细粒度特征在输入图中的来源相关性。</p><p><img src="/../imgs/Paper-Reading_Nested-Attention/image-20250108174006292.png" alt="可视化验证" title="可视化验证"></p>]]></content>
    
    
    <categories>
      
      <category>paper_reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>attention</tag>
      
      <tag>diffusion</tag>
      
      <tag>text-to-img</tag>
      
      <tag>personalization</tag>
      
      <tag>论文阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>影视剧、书籍、音乐和生活方式推荐</title>
    <link href="/My-world/My-World_Attitude/"/>
    <url>/My-world/My-World_Attitude/</url>
    
    <content type="html"><![CDATA[<p>影视剧、书籍、音乐和生活方式推荐</p><p>下面会介绍我推荐的、我最近在看的影视剧、书籍、音乐和生活方式，说实话我认为这种“生活态度”的安利是极具私人性的，毕竟，它们组成了我个人。不过我不介意，因为我更看重它们对我来说的其他意义：记忆和成长。因此我会持续更新的~ </p><p>不论是想要了解我还是想要接受我的安利，都请继续关注吧！ </p><h2 id="影视"><a href="#影视" class="headerlink" title="影视"></a>影视</h2><h3 id="电影"><a href="#电影" class="headerlink" title="电影"></a>电影</h3><ul><li><a href="https://movie.douban.com/subject/36445098/">还有明天</a> 2023<br>观影过程中以为和国内<a href="https://movie.douban.com/subject/36587974/">出走的决心-2024</a>剧情类似，结果！我还是太狭隘了！！强烈推荐</li><li><a href="https://movie.douban.com/subject/26656728/">老娘与海 <strong>又名：泳者之心</strong></a> 2024</li><li><a href="https://movie.douban.com/subject/25821498/">妇女参政论者</a> 2015</li></ul><h3 id="连续剧"><a href="#连续剧" class="headerlink" title="连续剧"></a>连续剧</h3><ul><li><a href="https://movie.douban.com/subject/1474087/">无耻之徒</a> 2004-2020 <strong>忽略frank的不负责任真的是最爱frank！</strong></li><li><a href="https://book.douban.com/subject/27204805/">我的天才女友(意大利)</a> 2018-2024<br>关键词：书籍《那不勒斯四部曲》改编：我的天才女友、新名字的故事、离开的，留下的、失踪的孩子<br>很细腻的女性作家书写的两名女性的友谊：复杂、既有爱又有恨和期待；很喜欢两个女生以各自的速度成长。</li><li><a href="https://movie.douban.com/subject/26838164/">伦敦生活</a> 2016-2019</li><li><a href="https://movie.douban.com/subject/36085524/">影后(台湾)</a> 2024 <strong>太爱杨谨华！</strong></li><li><a href="">人生复本</a>:看过<a href="">瑞克和莫蒂</a>的应该不会被这个电影惊艳到，’薛定谔的猫’系列科幻片 2025.2</li></ul><h3 id="动漫"><a href="#动漫" class="headerlink" title="动漫"></a>动漫</h3><ul><li>刺客伍六七：超级轻松搞笑！！！</li></ul><h2 id="书籍"><a href="#书籍" class="headerlink" title="书籍"></a>书籍</h2><ul><li><a href="https://book.douban.com/subject/25836270/">厌女：日本的女性厌恶——上野千鹤子</a> 2015<br>关键词：男性同性社会性欲望homosocial、厌女misogyny、同性恋憎恶homophobia、厌女症：男性对女性的蔑视和女性的自我厌恶<br>理论性还是比<a href="https://book.douban.com/subject/35523099/?icn=index-latestbook-subject">从零开始的女性主义</a>要高得多，而且翻译感好重，什么时候国内女性主义作家能把<a href="https://book.douban.com/subject/6722209/">男人之间</a>用国内的历史和语言文化，在近代的情况下进行更新的分析呢？希望！</li><li><a href="https://book.douban.com/subject/35966120/?icn=index-topchart-subject">始于极限——上野千鹤子</a> 2022 <strong>正在阅读</strong></li><li><a href="https://book.douban.com/subject/35143790/">蛤蟆先生去看心理医生——罗伯特·戴博德</a> 2020 <strong>第二次阅读</strong></li><li><a href="">双重赔偿</a> 2个小时就能看完的精彩小说！2025.2</li></ul><h2 id="音乐"><a href="#音乐" class="headerlink" title="音乐"></a>音乐</h2><ul><li>歌手：</li><li>别野加奈：很适合专注时的背景音会，让人平静下来</li></ul><h2 id="生活方式"><a href="#生活方式" class="headerlink" title="生活方式"></a>生活方式</h2><h3 id="播客："><a href="#播客：" class="headerlink" title="播客："></a>播客：</h3><ul><li>文化有限：每期对应一本<strong>书</strong></li><li>凹凸电波：搞笑的一群朋友们闲谈，听起来轻松</li><li>燕外之意：针对某一主题对网友的经历汇总</li><li>思文败类：轻松+小思考</li><li>随机波动：感性、深邃的女性在谈论中思考</li><li>岩中花述：鲁豫的对谈，最近都是”she”她主题的女性嘉宾</li></ul><h3 id="运动："><a href="#运动：" class="headerlink" title="运动："></a>运动：</h3><ul><li>健身：争取引体向上</li><li>攀岩：新时代不分性别的”裹小脚”</li><li>网球：希望对球的控制更稳定一些</li><li>游泳：谁能想到每周都在坚持游泳！</li></ul><p><img src="/../imgs/roy.jpg" alt="王源图片" title="roy"></p><blockquote><p>github: <a href="https://pljj315.github.io/">pljj315</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>My world</category>
      
    </categories>
    
    
    <tags>
      
      <tag>安利</tag>
      
      <tag>影视剧</tag>
      
      <tag>书籍</tag>
      
      <tag>音乐</tag>
      
      <tag>生活方式</tag>
      
      <tag>recommends</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>paper reading:  IDM-VTON: Improving Diffusion Models for Authentic Virtual Try-on in the Wild-2024 ECCV</title>
    <link href="/paper-reading/Paper-Reading_IDM-Virtual-Try-on/"/>
    <url>/paper-reading/Paper-Reading_IDM-Virtual-Try-on/</url>
    
    <content type="html"><![CDATA[<h2 id="IDM-VTON-Improving-Diffusion-Models-for-Authentic-Virtual-Try-on-in-the-Wild-in-2024-ECCV"><a href="#IDM-VTON-Improving-Diffusion-Models-for-Authentic-Virtual-Try-on-in-the-Wild-in-2024-ECCV" class="headerlink" title="IDM-VTON: Improving Diffusion Models for Authentic Virtual Try-on in the Wild (in 2024 ECCV)"></a>IDM-VTON: Improving Diffusion Models for Authentic Virtual Try-on in the Wild (in 2024 ECCV)</h2><p>论文概括：论文中提出了一种改进的扩散模型 (Improved Diffusion Models for Virtual Try-ON, 简称IDM–VTON)，使其应用在真实世界场景下虚拟试穿任务。该模型显著提高了服装图像的一致性，可以生成真实的虚拟试穿图像。具体地，设计一个复杂的注意力模块(attention modules)，可以将服装图像更好的编码到扩散模型，该注意力模块由两个不同的组件组成：</p><ul><li>作用在self-attention——IP-Adapter：对图片形式的服装提示进行编码适配(image prompt adapter)【高级语义】</li><li>作用在cross-attention——GarmentNet：同样对服装图片进行编码，不过使用UNet的hidden_state直接得到特征，保留细粒度的细节【低层特征】</li></ul><span id="more"></span><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="IDM-VTON整个模型框架由三个部分组成："><a href="#IDM-VTON整个模型框架由三个部分组成：" class="headerlink" title="IDM-VTON整个模型框架由三个部分组成："></a>IDM-VTON整个模型框架由三个部分组成：</h4><ul><li>TryonNet：13通道的UNet，以常见的9通道InpaintingModel作为初始化+4通道的pose map latent</li><li>IP-Adapter：提取服装高级语义**【代码中并没有训练？】**</li><li>GarmentNet：基于UNet的特征编码器，提取服装的低级特征，直接使用冻结的SDXL-base</li></ul><p><img src="/../imgs/Paper-Reading_IDM-Virtual-Try-on/image-20250222211416822.png" alt="IDM-VTON模型整体框架"></p><h4 id="1-TryonNet-详解"><a href="#1-TryonNet-详解" class="headerlink" title="1. TryonNet 详解"></a>1. TryonNet 详解</h4><p>TryonNet的输入由4个部分组成，共13通道&#x3D;4+4+4+1：除了单通道的mask外，均为4个通道（VAE编码层获得latent是4通道）：使用开源的9通道SDXL-Inpainting模型作为初始化，并使用卷积层来修改UNet的输入特征通道（卷积层初始化为0）。代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">unet.encoder_hid_proj = image_proj_model<br><br>conv_new = torch.nn.Conv2d(<br>    in_channels=<span class="hljs-number">4</span>+<span class="hljs-number">4</span>+<span class="hljs-number">1</span>+<span class="hljs-number">4</span>,<br>    out_channels=unet.conv_in.out_channels,<br>    kernel_size=<span class="hljs-number">3</span>,<br>    padding=<span class="hljs-number">1</span>,<br>)<br>torch.nn.init.kaiming_normal_(conv_new.weight)  <br>conv_new.weight.data = conv_new.weight.data * <span class="hljs-number">0.</span>  <span class="hljs-comment"># 初始化为0 </span><br><br>conv_new.weight.data[:, :<span class="hljs-number">9</span>] = unet.conv_in.weight.data  <br>conv_new.bias.data = unet.conv_in.bias.data  <br><br>unet.conv_in = conv_new  <span class="hljs-comment"># replace conv layer in unet</span><br>unet.config[<span class="hljs-string">&#x27;in_channels&#x27;</span>] = <span class="hljs-number">13</span>  <span class="hljs-comment"># update config</span><br>unet.config.in_channels = <span class="hljs-number">13</span>  <span class="hljs-comment"># update config</span><br><span class="hljs-comment">#customize unet end</span><br></code></pre></td></tr></table></figure><h4 id="2-IP-Adapter-详解"><a href="#2-IP-Adapter-详解" class="headerlink" title="2. IP-Adapter 详解"></a>2. IP-Adapter 详解</h4><p>使用IP-Adapter对服装图像进行编码，使用冻结参数的CLIP图像编码器（即 OpenCLIP ViT-H&#x2F;14）来提取特征。论文说IP-Adapter【包括特征投影层（feature projection layers）和交叉注意层（cross-attention layers）】是可训练的，<em>但代码里并没有训练</em>，社区的issues也提到了不训练可能会更好：ipa可能会影响到衣服颜色准确度*。 </p><p><img src="/../imgs/Paper-Reading_IDM-Virtual-Try-on/image-20250222213603226.png" alt="原版IP-Adapter论文图片"></p><h4 id="3-GarmentNet-详解"><a href="#3-GarmentNet-详解" class="headerlink" title="3. GarmentNet 详解"></a>3. GarmentNet 详解</h4><ul><li><p>由于 <strong>CLIP 图像编码器缺乏提取服装的低级特征（CLIP训练的文本图像对比较广泛，对服装细节无法详细的描述）</strong>，导致：虽然IP-Adapter已经引入了来自服装图像的控制，但当服装具有复杂的图案或印花时，IP-Adapter在保留服装的细粒度细节方面存在不足。而SDXL模型在大规模数据集上经过预训练，已经具备了良好的低级特征提取器的能力，可以作为对IPA的细粒度特征的补充特征。</p></li><li><p>具体实现可以理解为：来自两个UNet网络的低层次特征 (Low-level features) 先通过<strong>自注意力</strong>进行特征融合，再和来自IP-Adapter的高级语义（High-level semantics）通过<strong>交叉注意力</strong>进行特征融合</p><ul><li><strong>冻结SDXL-base的UNet网络参数</strong>，并获取其hidden state中间表示作为低级特征</li><li>与来自TryonNet的中间表达进行连接，然后计算连接特征的自注意力(Self Attention)</li><li>只传递来自TryonNet的前半部分特征，将其与IP-Adapter结果特征进行交叉注意力(Cross Attention)计算并继续传递</li></ul></li></ul><h4 id="4-使用更详细的Caption：对服装的细节进行精细描述"><a href="#4-使用更详细的Caption：对服装的细节进行精细描述" class="headerlink" title="4. 使用更详细的Caption：对服装的细节进行精细描述"></a>4. 使用更详细的Caption：对服装的细节进行精细描述</h4><p>更详细的描述有助于模型使用自然语言对服装的高级语义进行编码，对基于图像的信息进行补充。[V]，即 cloth_annotation 通过图像注释器（image annotator，对图片中的特定属性进行分类识别）获得。</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs applescript">替换公式：<br>[V] = short sleeve <span class="hljs-built_in">round</span> neck t-shirts<br>A photo <span class="hljs-keyword">of</span> [V] = A photo <span class="hljs-keyword">of</span> short sleeve <span class="hljs-built_in">round</span> neck t-shirts<br>Model <span class="hljs-keyword">is</span> wearing [V] = Model <span class="hljs-keyword">is</span> wearing short sleeve <span class="hljs-built_in">round</span> neck t-shirts<br><br>代码：<br><span class="hljs-literal">result</span>[<span class="hljs-string">&quot;caption&quot;</span>] = <span class="hljs-string">&quot;model is wearing &quot;</span> + cloth_annotation<br><span class="hljs-literal">result</span>[<span class="hljs-string">&quot;caption_cloth&quot;</span>] = <span class="hljs-string">&quot;a photo of &quot;</span> + cloth_annotation<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python">annotation_list = [ <span class="hljs-comment"># </span><br>    <span class="hljs-comment"># &quot;colors&quot;,</span><br>    <span class="hljs-comment"># &quot;textures&quot;,</span><br>    <span class="hljs-string">&quot;sleeveLength&quot;</span>,<br>    <span class="hljs-string">&quot;neckLine&quot;</span>,<br>    <span class="hljs-string">&quot;item&quot;</span>,<br>]<br><br><span class="hljs-variable language_">self</span>.annotation_pair = &#123;&#125;<br><span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> data1.items():<br>    <span class="hljs-keyword">for</span> elem <span class="hljs-keyword">in</span> v:<br>        annotation_str = <span class="hljs-string">&quot;&quot;</span><br>        <span class="hljs-keyword">for</span> template <span class="hljs-keyword">in</span> annotation_list: <span class="hljs-comment"># 遍历 annotation_list，依次提取每一项的分类结果，对字符串进行连接获得最终描述</span><br>            <span class="hljs-keyword">for</span> tag <span class="hljs-keyword">in</span> elem[<span class="hljs-string">&quot;tag_info&quot;</span>]:<br>                <span class="hljs-keyword">if</span> (<br>                    tag[<span class="hljs-string">&quot;tag_name&quot;</span>] == template<br>                    <span class="hljs-keyword">and</span> tag[<span class="hljs-string">&quot;tag_category&quot;</span>] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>                ):<br>                    annotation_str += tag[<span class="hljs-string">&quot;tag_category&quot;</span>]<br>                    annotation_str += <span class="hljs-string">&quot; &quot;</span><br>        <span class="hljs-variable language_">self</span>.annotation_pair[elem[<span class="hljs-string">&quot;file_name&quot;</span>]] = annotation_str<br></code></pre></td></tr></table></figure><hr>]]></content>
    
    
    <categories>
      
      <category>paper_reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>attention</tag>
      
      <tag>diffusion</tag>
      
      <tag>text-to-img</tag>
      
      <tag>personalization</tag>
      
      <tag>论文阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
